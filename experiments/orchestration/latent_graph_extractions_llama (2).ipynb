{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "from torch import device\n",
    "from tqdm import tqdm\n",
    "from tooling.agents.data import load_csv_file, load_json_file\n",
    "from time import time\n",
    "from pandas import DataFrame\n",
    "import pickle\n",
    "from argparse import ArgumentParser\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# from tooling.llm_engine.gorilla2_function import GorillaFunctionCalling as llm\n",
    "# from tooling.llm_engine.gorilla2_function import Llama3_18BInstruct as llm\n",
    "from tooling.huggingface_latent_representations.transformers.attention_featurization import extract_latent_feature_graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PLATFORM_URL']='ml-platform-cyclops.dev.svc.splunk8s.io'\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"3bdc2fa093ce487794e774335415137b\"\n",
    "os.environ[\"ML_PLATFORM_AUTH_TOKEN\"] = \"eyJhbGciOiJSUzI1NiIsImtpZCI6Ii8xSGxtVEZ4STRZaC8remhsNVZRQ214R2RKY0p0cnpXY2NSdS9YWVBTL3MiLCJ0eXAiOiJKV1QifQ.eyJhdWQiOlsiYXBpOi8vc2NzLWRlZmF1bHQiXSwiY2lkIjoiMG9hMWU2NnB5ZXpjcTVjdzEweDciLCJleHAiOjE3MzkzNDE1NjgsImdyb3VwcyI6WyJFdmVyeW9uZSIsInNnLXNzYy1ldmVyeW9uZSIsInNnLXNwbHVuay1kZWZhdWx0Iiwic2ctc3NjLW1sLXNlcnZpY2VzIiwic2ctYXV0b21hdGVkLWdpdGxhYi1hY2Nlc3MiLCJzZy1hdXRvbWF0ZWQtazhzLWFjY2VzcyIsInNnLWF1dG9tYXRlZC1jbzItc3RhZ2luZyIsInNnLWF1dG9tYXRlZC1jbzItZGV2Iiwic2ctY2xvdWQtZW5nLWV2ZXJ5b25lIiwic2ctYXV0b21hdGVkLXNmeC1zY3MtYWNjZXNzIiwic2ctcGZtLW1sIiwic2ctcGZtLWV2ZXJ5b25lIiwic2ctY2xvdWQtZW5nLWZ0ZSIsInNnLXNwbHVuay1lbmctZXZlcnlvbmUiLCJzZy1hdXRvbWF0ZWQtYXdzLWNsaS1hY2Nlc3MiLCJzZy1jbG91ZC1haS1henVyZS1vcGVuYWktZGV2Il0sImlhdCI6MTczOTI5ODM2OSwiaWRwIjoic2NvLnNhbWwuaWRwLmlkIiwiaXNzIjoiaHR0cHM6Ly9hdXRoLnBsYXlncm91bmQuc2NzLnNwbHVuay5jb20iLCJqdGkiOiJjdWxwY2diZDZ0NTJudXFmOGhtZyIsInN1YiI6InByYWNoaXBhdGVsQHNwbHVuay5jb20iLCJ0ZW5hbnQiOiJjc2xvdHRlcmJhY2sifQ.WnXOEeyx8se-Qey0jJW8Gba8dzI1JxDq63YXUe9_sb9onB3E8aMkshPodzQtUZFxpCdTzYLtD5Q1R7SLfE_wGD-Dp3Jv_-GWFXQzqY_6DCKpLCcrlU9k43q7y14g5fxg_Zxrgvvmy6tQzqtxX0evh_CdvaUUbXAXLtlElBaTDTwTKu0PiEr9CfMaKsG9H3icRY4IYBBJYHW088TDutN7sl45_FZi1jVk--vJgWB4oQVGn3WPUlNU7o8Z1Ns81xCFnhimVWJmdb7Hz9AUEor3W4HgQK7YU9-F_ni9_wZ6ePHpRKhtZ1BM1HeuUdgfL7PNYdzi7WfNV15ndVKz4L-8-g\"\n",
    "os.environ[\"SCS_TOKEN\"]=os.environ[\"ML_PLATFORM_AUTH_TOKEN\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "     |████████████████████████████████| 1.3 MB 2.3 MB/s            \n",
      "\u001b[?25hInstalling collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.2.0\n"
     ]
    }
   ],
   "source": [
    "# !pip install sentencepiece\n",
    "hf_token='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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Device: NVIDIA A10G\n",
      "Total GPU Memory: 23.70 GB\n",
      "Allocated Memory: 0.00 GB\n",
      "Cached Memory: 0.00 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Total GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    print(f\"Allocated Memory: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB\")\n",
    "    print(f\"Cached Memory: {torch.cuda.memory_reserved(0) / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"CUDA is not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/.local/lib/python3.9/site-packages/transformers/models/auto/tokenization_auto.py:823: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e236d427d41745b79e89a1a5f1830027",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/50.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ce64b0f3ec24b48a0c2d8a27db94efa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01f46369c74047d7ab64c6bdc91bfd1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/73.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/.local/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87555c282e6d455db68b9c8f8ded15e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/826 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faa53ae50a654ec797fea1bcd98c5aee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42337fbdfeac4bd0940856491615419b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "558e6a7a4d084fddad77124d32a715a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d75785561c1b469c99250bc1c289d960",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b667c7ca0c54ed4a646d8e3bf324506",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f985add3553f48cb993882f7ee14bc0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/.local/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:818: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_attentions` is. When `return_dict_in_generate` is not `True`, `output_attentions` is ignored.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e1fe1fc049347b2aaad128be08f9684",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7cd6692e60f4035a9b90a28e2235a32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/185 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "model_name = \"meta-llama/Llama-3.1-8B\" # Replace with \"llama31_8b\" or your specific model\n",
    "#model_name = \"meta-llama/llama31_8b\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name,use_auth_token=hf_token)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, output_attentions=True, torch_dtype=torch.float16, device_map=\"auto\",\\\n",
    "                                             use_auth_token=hf_token)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup system prompt, user query or chat, and get output token's weight\n",
    "orch_prompt = '''You are a technical assistant, and you are a Splunk expert. You will evaluate each 'user' query, user's chat history and tool information to determine which tool/skill from Security assistant is fit to fulfill user's request.\n",
    "Only respond with tool/skill name and nothing more.\n",
    "\n",
    "List of skills/tools available to you are: investigation_report, conversation_response, spl_writer, findings_summarizer. \n",
    "\n",
    "You do not respond as 'User' or pretend to be 'User'. You only respond once as 'Assistant'. Respond just tool/skill name based on the tool instructions provided below, user's chat history and user's query. \n",
    "Stop on the first request, do not continue the conversation.\n",
    "\n",
    "User Chat history also contains the tool/skill name which were ran, as part of assistant response.\n",
    "\n",
    "### Tool Guidance\n",
    "You have access to three security tools:\n",
    "1) Tool name: spl_writer \n",
    "spl_writer - tool description: Generates Splunk SPL queries that achieve the user's intent. SPL can access Splunk data like logs, raw events, host telemetry, emails, users, and much more.\n",
    "spl_writer - example user requests:\n",
    "    - Write spl to list available source_types.\n",
    "    - Find the user accounts associated with failed login attempts.\n",
    "    - Spl query to see high risk events in last 24hrs.\n",
    "    - Write SPL to list all indices\n",
    "    - Update the SPL to use host=localhost\n",
    "    - How many events are in the main index\n",
    "    - Who has signed into host abc in the last 2 days\n",
    "    \n",
    "2) Tool name: findings_summarizer\n",
    "findings_summarizer - tool description: Provides a concise summary of security findings. Note: This tool generates static summaries, which are generally consistent throughout the session.\n",
    "findings_summarizer - example user requests:\n",
    "    - Summarize the [alert/investigation].\n",
    "    - Give a short description of all findings.\n",
    "    - Summarize findings.\n",
    "    - Explain the Findings.\n",
    "    - Explain the Findings of this investigation.\n",
    "    - What are the findings in this investigation?\n",
    "    - Give me the summary of this finding.\n",
    "    - Generate the summary of these findings.\n",
    "    - What do the findings in this investigation show?\n",
    "    - What do the findings in this investigation say?\n",
    "    -AS A STRICT RULE, if the input user query has the word \"\"summary\"\" in it, return \"\"findings_summary\"\" tool\n",
    "\n",
    "3) Tool name: investigation_report\n",
    "investigation_report - tool description: Creates detailed reports of investigations, including event timelines and comprehensive information. Note: This tool generates static reports, which are generally consistent throughout the session.\n",
    "investigation_report - example user requests:\n",
    "    - Give me the details about this incident.\n",
    "    - List all findings for a given incident.\n",
    "    - Generate investigation report\n",
    "    - report\n",
    "    - give me the report\n",
    "    - give me timeline of events\n",
    "    - generate timeline for this investigation\n",
    "    - AS A STRICT RULE, if the input user query has the word \"\"report\"\" in it, return \"\"investigation_report\"\" tool\n",
    "\n",
    "You also have access to a system tools:\n",
    "4) Tool name: conversation_response \n",
    "conversation_response - tool descriptionResponds to user queries about investigation objects, chat history references, or general questions. The conversation skill can also answer questions about the investigation's object fields, such as:\n",
    "    • create_time, update_time, id, description, incident_type, status, assignee, urgency, sensitivity, and others.\n",
    "conversation_response - example user requests:\n",
    "    - What are your skills?\n",
    "    - What does spl mean?\n",
    "    - How do i write an investigation report?\n",
    "    - What is a finding in the context of Splunk?\n",
    "    - What does this spl query mean?\n",
    "    - What is the urgency of this investigation?\n",
    "    - Who is the assignee for this case?\n",
    "\n",
    "Further instructions for you : \n",
    "\n",
    "If the Agent stopped due to iteration limit or time limit, still summarize what you did so far.\n",
    "You MUST ALWAYS respond with the `spl_writer` tool to write SPL.\n",
    "You MUST ALWAYS use the `conversation_response` tool to respond to user requests if no other security tool/skill applies.\n",
    "\n",
    "Do not include optional arguments in the tool call if their value is not in query or chat history. Do not generate text to use as a value, instead do not include optional argument in tool call.\n",
    "AS A STRICT RULE, Always respond with only the skill/tool name from the skill choices of: investigation_report, conversation_response, spl_writer, findings_summarizer.\n",
    "AS A STRICT RULE, please do not output any additional text. ONLY give name of the tool without any explanation.\n",
    "Focus more on the last user query to identify the correct tool for the last user query. Use the chat history for context.\n",
    " \n",
    "Read the user intent, chat history provided below under 'user' and list of tools & their descriptions provided above to pick the right tool/skill to use amongst [investigation_report, conversation_response, spl_writer, findings_summarizer]'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example usage (adapt as needed for your specific tool usage scenario)\n",
    "\n",
    "# # You should only return the function call in tools call sections.\n",
    "\n",
    "# # If you decide to invoke any of the function(s), you MUST put it in the format of [func_name1(params_name1=params_value1, params_name2=params_value2...), func_name2(params)]\n",
    "# # You SHOULD NOT include any other text in the response.\n",
    "# # Here is a list of functions in JSON format that you can invoke.\\n{functions}\\n\n",
    "# # \"\"\"\n",
    "# User query\n",
    "\n",
    "\n",
    "tools = [\n",
    "    {\n",
    "        \"name\": \"finding_summarizer\",\n",
    "        \"description\": \"Findings Summarizer skill is built for summarizing or in other words explaining the meaning of findings found in an investigation. User query may have the words 'finding' or 'summarize' or 'summary' in it.\",\n",
    "        \"examples\": [\"summarize this incident\", \"write a summary of this incident\", \"summarize findings\", \"explain the findings\", \"list findings\", \"explain the detections that triggered this finding\", \"what is the meaning of this incident\", \"Can you simplify this (json)\"],\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"user_input\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"User query asking a summary of the incident. User query may have the word summary or summarize or findings in it.\"\n",
    "                },\n",
    "                \"output\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"A natural language summary of the incident.\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": []\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"investigation_report\",\n",
    "        \"description\": \"Investigation report skill is built to generate a report for the investigation including key information that gives a mid to high level overview of the incident. It gives a chronological timeline of events associated with the incident including the dates, times and descriptions of activity for investigation creation, finding discovery and notes being added by the analyst. User query may have the word 'report' in it.\",\n",
    "        \"examples\": [\"write a report\", \"Generate a report\", \"Give me timeline of events\",\"Provide a timeline of key events from analyst notes\", \"report\", \"make an investigation report\", \"generate final investigation report for incident\"],\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"user_input\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"User query asking to generate a report.  User query may have the word 'report' in it.\"\n",
    "                },\n",
    "                \"output\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"A splunk incident report.\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": []\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"conversation_response\",\n",
    "        \"description\": \"Takes a user's request which is not handled by ['finding_summarizer', 'investigation_report', 'spl_writer'] and responds with security relevant information and pertinent information already in the chat history. User may ask questions about the investigation data.\",\n",
    "        \"examples\": [\"as9kx\", \"why do we exist?\", \"Hi how are you?\", \"Are you an AI?\", \"I see you are a bot\", \"explain splunk\", \"what can you do with spl\", \"3 + 3\", \"what are your skills?\", \"help\", \"what is the time of first finding?\", \"explain the detections that triggered this finding\", \"what is the status of this investigation?\", \"give the number of findings in last investigation\"],\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"user_input\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"User query with general question or question about value of specific data in the investigation. If user's query doesn't match any other function, pick conversation_response.\"\n",
    "                },\n",
    "                \"output\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"Response answering the user question.\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": []\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"spl_writer\",\n",
    "        \"description\": \"User input is either natural language or SPL query. Converts a natural language request into an SPL query that can be executed to fulfill the user's intent. User input can be an ask for data analysis, show data insights, creation of data entries, find information from data, data manipulation action, monitor activity, replace data, extract or give entities, show or display or get events/entities, extracting event data from last 24 hours or a time window, or an ask to write code. User input may also ask to modify the previous query or add something to previous query.\",\n",
    "        \"examples\": [\"List available indices\", \"your previous SPL did not work\",\"Create a record with a value of 'abcd123'\",\"get me all risk entities related to this incident\", \"give me another query for this task\", \"top 10 IP addresses by count\",\" all error messages in the logs from yesterday\",\"Show me all the notables for this host\",\"average daily ingestion per month\", \"previous SPL is wrong\",\"show me all notables for this host in the last 60 days\",\"Find accounts associated with failed login attempts.\",\"check _key field in lookup\",\"INFO  CMBucket  Freezing bid\",\"list all the hostnames in my deployment\",\"who are the users associated with login attempts for linux in past 24 hours\",\"list sources that are consuming a lot of GPU RAM\",\"search aws access logs last 3 errors\",\"show me all the indexes that do not have any logs\", \"get risk events in last 24 hours\", \"give me users associated with risk event in past 24 hours\", \"replace the risk object\", \"add user=ax to the last spl query\", \"update the search to look for id=123\"],\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"intent\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"A natural language description of the user's intent or request. User may ask to modify or add to past SPL query.\"\n",
    "                },\n",
    "                \"output\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The SPL query that corresponds to the user's request.\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"intent\"]\n",
    "        }\n",
    "    }\n",
    "]\n",
    "def out_sys_prompt(tools):\n",
    "    return f\"\"\"\n",
    "    You are an expert in picking functions. You are given a question/statement and a set of possible functions.\n",
    "    Based on the question/statement, you will need to make one function to achieve the purpose in question/statement.\n",
    "    You do not provide any explanation, additional text, or answers to user's input as your output. \n",
    "    If the input is correct or incorrect SPL query, pick the function spl_writer. \n",
    "    If the input asks you to create data entries, find from data, do data manipulation or data parsing, show or display data insights, write code, update or add to previous SPL query or asks if the past query was incorrect, pick the function spl_writer. \n",
    "    If the input is gibbrish, pick the function conversation_response. \n",
    "    AS A STRICT RULE, do not answer any of the user questions and STRICTLY only output the function for the user's query from this list of functions:investigation_report, conversation_response, spl_writer, finding_summarizer\"\n",
    "    STRICTLY follow this output format for your response:\n",
    "    Example query: generate report\n",
    "    Your output: investigation_report\n",
    "    Strictly only output the picked function name without any additional text.\n",
    "    Here is a list of functions in JSON format that you can invoke.  \n",
    "    \n",
    "    ### Tool Guidance: \\n\n",
    "\n",
    "You have access to four tools: \n",
    "\n",
    "    {tools}\\n\n",
    "    \"\"\"\n",
    "orch_prompt=out_sys_prompt(tools)\n",
    "#system_prompt.format(functions=tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_llama31_input( prompt, query):\n",
    "    return f\"\"\"\n",
    "    <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "\n",
    "    {prompt}<|eot_id|>\n",
    "    <|start_header_id|>user<|end_header_id|>\n",
    "\n",
    "    {query}<|eot_id|>\n",
    "    <|start_header_id|>assistant<|end_header_id|>\n",
    "    \"\"\".strip()\n",
    "\n",
    "def run_llama31_instruct(prompt, query, model_url):\n",
    "    headers = {\"Authorization\": f\"Bearer {os.environ['SCS_TOKEN']}\"}\n",
    "    prompt = generate_llama31_input(prompt, query)\n",
    "    payload = {\"prompt\": prompt, \"config\": {\"max_new_tokens\": 8, \"temperature\":0}}\n",
    "    response = requests.post(model_url, json=payload, headers=headers, stream=True)\n",
    "    chunks = list(response.iter_content(chunk_size=None))\n",
    "    report = \"\".join(chunk.decode(\"utf8\") for chunk in chunks)\n",
    "    report_dict = json.loads(report)\n",
    "    return report_dict.get(\"generated_text\") \n",
    "\n",
    "\n",
    "# LLAMA33_70B_URL = 'https://ml-platform-cyclops.dev.svc.splunk8s.io/llama33_70b_instruct_awq/generate'\n",
    "# messages = 'Hi'\n",
    "# response = run_llama31_instruct(sys_prompt, messages, LLAMA33_70B_URL).strip()\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear cache and synchronize\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0.1  Unnamed: 0  \\\n",
      "0             0           0   \n",
      "1             1           1   \n",
      "2             2           2   \n",
      "3             3           3   \n",
      "4             4           4   \n",
      "\n",
      "                                           messsages  \\\n",
      "0  [{'role': 'system', 'content': \"You are a tech...   \n",
      "1  [{'role': 'system', 'content': \"You are a tech...   \n",
      "2  [{'role': 'system', 'content': \"You are a tech...   \n",
      "3  [{'role': 'system', 'content': \"You are a tech...   \n",
      "4  [{'role': 'system', 'content': \"You are a tech...   \n",
      "\n",
      "                        latest_msg            Human_label  \\\n",
      "0                              Hi!  conversation_response   \n",
      "1            What are your skills?  conversation_response   \n",
      "2                  What is splunk?  conversation_response   \n",
      "3              What does spl mean?  conversation_response   \n",
      "4  What is the weather like today?  conversation_response   \n",
      "\n",
      "     reference_skill_gpt data_source                              llama31_70b  \\\n",
      "0  conversation_response         poc  {'skill_name': 'conversation_response'}   \n",
      "1  conversation_response         poc  {'skill_name': 'conversation_response'}   \n",
      "2  conversation_response         poc  {'skill_name': 'conversation_response'}   \n",
      "3  conversation_response         poc                    conversation_response   \n",
      "4  conversation_response         poc                    conversation_response   \n",
      "\n",
      "                                                 gpt  match  ...  \\\n",
      "0  name='conversation_response' arguments={'inten...      1  ...   \n",
      "1  name='conversation_response' arguments={'inten...      1  ...   \n",
      "2  name='conversation_response' arguments={'inten...      1  ...   \n",
      "3  name='conversation_response' arguments={'inten...      1  ...   \n",
      "4  name='conversation_response' arguments={'excep...      1  ...   \n",
      "\n",
      "   human_response_normalized      llama33_responses  \\\n",
      "0      conversation_response  conversation_response   \n",
      "1      conversation_response  conversation_response   \n",
      "2      conversation_response  conversation_response   \n",
      "3      conversation_response  conversation_response   \n",
      "4      conversation_response  conversation_response   \n",
      "\n",
      "   llama33_responses_latency  llama33_response_normalized  \\\n",
      "0                   5.029794        conversation_response   \n",
      "1                   4.684258        conversation_response   \n",
      "2                   5.145237        conversation_response   \n",
      "3                   5.326115        conversation_response   \n",
      "4                   6.585232        conversation_response   \n",
      "\n",
      "   llama33_newp_responses llama33_newp_responses_latency  \\\n",
      "0   conversation_response                       3.682450   \n",
      "1   conversation_response                       3.303961   \n",
      "2   conversation_response                       3.693146   \n",
      "3   conversation_response                       3.766858   \n",
      "4   conversation_response                       5.135024   \n",
      "\n",
      "  llama33_newp_responses_normalized  llama33_tkn8_responses  \\\n",
      "0             conversation_response   conversation_response   \n",
      "1             conversation_response   conversation_response   \n",
      "2             conversation_response   conversation_response   \n",
      "3             conversation_response   conversation_response   \n",
      "4             conversation_response   conversation_response   \n",
      "\n",
      "  llama33_tkn8_responses_latency latest_msg_normalized  \n",
      "0                       3.674159                    hi  \n",
      "1                       3.286413                 skill  \n",
      "2                       3.777983                splunk  \n",
      "3                       3.891352              spl mean  \n",
      "4                       5.119708    weather like today  \n",
      "\n",
      "[5 rows x 29 columns]\n",
      "['conversation_response', 'conversation_response or findings_summarizer', 'conversation_response or spl_writer', 'finding_summarizer', 'investigation_report', 'spl_writer']\n",
      "{'conversation_response': 0, 'conversation_response or findings_summarizer': 1, 'conversation_response or spl_writer': 2, 'finding_summarizer': 3, 'investigation_report': 4, 'spl_writer': 5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6642/3043198294.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  labels[i]=\"finding_summarizer\"\n",
      "/tmp/ipykernel_6642/3043198294.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  labels[i]=\"finding_summarizer\"\n",
      "/tmp/ipykernel_6642/3043198294.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  labels[i]=\"finding_summarizer\"\n",
      "/tmp/ipykernel_6642/3043198294.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  labels[i]=\"finding_summarizer\"\n",
      "/tmp/ipykernel_6642/3043198294.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  labels[i]=\"finding_summarizer\"\n",
      "/tmp/ipykernel_6642/3043198294.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  labels[i]=\"finding_summarizer\"\n",
      "/tmp/ipykernel_6642/3043198294.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  labels[i]=\"finding_summarizer\"\n",
      "/tmp/ipykernel_6642/3043198294.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  labels[i]=\"finding_summarizer\"\n",
      "/tmp/ipykernel_6642/3043198294.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  labels[i]=\"finding_summarizer\"\n",
      "/tmp/ipykernel_6642/3043198294.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  labels[i]=\"finding_summarizer\"\n",
      "/tmp/ipykernel_6642/3043198294.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  labels[i]=\"finding_summarizer\"\n",
      "/tmp/ipykernel_6642/3043198294.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  labels[i]=\"finding_summarizer\"\n",
      "/tmp/ipykernel_6642/3043198294.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  labels[i]=\"finding_summarizer\"\n",
      "/tmp/ipykernel_6642/3043198294.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  labels[i]=\"finding_summarizer\"\n",
      "/tmp/ipykernel_6642/3043198294.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  labels[i]=\"finding_summarizer\"\n",
      "/tmp/ipykernel_6642/3043198294.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  labels[i]=\"finding_summarizer\"\n",
      "/tmp/ipykernel_6642/3043198294.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  labels[i]=\"finding_summarizer\"\n"
     ]
    }
   ],
   "source": [
    "# !export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True\n",
    "# import pandas as pd\n",
    "# df = pd.read_csv('~/saia-finetuning-main/experiments/orchestration/orchestrator_data/orch_data_expt.csv')\n",
    "# load orchestrator data\n",
    "import pandas as pd\n",
    "data=pd.read_csv(\"../../grading/data/orchestrator_final.csv\")\n",
    "print(data.head())\n",
    "labels = data['Human_label']\n",
    "input_data = data['latest_msg']\n",
    "\n",
    "for i in range(len(labels)):\n",
    "    if labels[i]==\"summerize_findings\":\n",
    "        labels[i]=\"finding_summarizer\"\n",
    "    if labels[i]==\"findings_summerizer\":\n",
    "        labels[i]=\"finding_summarizer\"  \n",
    "    if labels[i]==\"finding_summary\":\n",
    "        labels[i]=\"finding_summarizer\"\n",
    "    # if labels[i]==\"spl or conversation?\":\n",
    "    #     labels[i]=\"spl_writer\"\n",
    "    # if labels[i]==\"spl writer or conversation?\":\n",
    "    #     labels[i]=\"spl_writer\"\n",
    "responses = []\n",
    "graph_latency = []\n",
    "response_latency = []\n",
    "unique_lab=sorted(set(labels))\n",
    "print(unique_lab)\n",
    "\n",
    "label_mapping = {value: idx for idx, value in enumerate(unique_lab)}\n",
    "print(label_mapping)\n",
    "attention_graphs = []\n",
    "\n",
    "# model = llm()\n",
    "limit = len(data)\n",
    "del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['msg_len'] = df.modified_msg.apply(lambda x:len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df[['msg_len','messsages']].sort_values(by='msg_len')[49]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# messages = df.enriched_msgs[49]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi!\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "    \n",
      "    You are an expert in picking functions. You are given a question/statement and a set of possible functions.\n",
      "    Based on the question/statement, you will need to make one function to achieve the purpose in question/statement.\n",
      "    You do not provide any explanation, additional text, or answers to user's input as your output. \n",
      "    If the input is correct or incorrect SPL query, pick the function spl_writer. \n",
      "    If the input asks you to create data entries, find from data, do data manipulation or data parsing, show or display data insights, write code, update or add to previous SPL query or asks if the past query was incorrect, pick the function spl_writer. \n",
      "    If the input is gibbrish, pick the function conversation_response. \n",
      "    AS A STRICT RULE, do not answer any of the user questions and STRICTLY only output the function for the user's query from this list of functions:investigation_report, conversation_response, spl_writer, finding_summarizer\"\n",
      "    STRICTLY follow this output format for your response:\n",
      "    Example query: generate report\n",
      "    Your output: investigation_report\n",
      "    Strictly only output the picked function name without any additional text.\n",
      "    Here is a list of functions in JSON format that you can invoke.  \n",
      "    \n",
      "    ### Tool Guidance: \n",
      "\n",
      "\n",
      "You have access to four tools: \n",
      "\n",
      "    [{'name': 'finding_summarizer', 'description': \"Findings Summarizer skill is built for summarizing or in other words explaining the meaning of findings found in an investigation. User query may have the words 'finding' or 'summarize' or 'summary' in it.\", 'examples': ['summarize this incident', 'write a summary of this incident', 'summarize findings', 'explain the findings', 'list findings', 'explain the detections that triggered this finding', 'what is the meaning of this incident', 'Can you simplify this (json)'], 'parameters': {'type': 'object', 'properties': {'user_input': {'type': 'string', 'description': 'User query asking a summary of the incident. User query may have the word summary or summarize or findings in it.'}, 'output': {'type': 'string', 'description': 'A natural language summary of the incident.'}}, 'required': []}}, {'name': 'investigation_report', 'description': \"Investigation report skill is built to generate a report for the investigation including key information that gives a mid to high level overview of the incident. It gives a chronological timeline of events associated with the incident including the dates, times and descriptions of activity for investigation creation, finding discovery and notes being added by the analyst. User query may have the word 'report' in it.\", 'examples': ['write a report', 'Generate a report', 'Give me timeline of events', 'Provide a timeline of key events from analyst notes', 'report', 'make an investigation report', 'generate final investigation report for incident'], 'parameters': {'type': 'object', 'properties': {'user_input': {'type': 'string', 'description': \"User query asking to generate a report.  User query may have the word 'report' in it.\"}, 'output': {'type': 'string', 'description': 'A splunk incident report.'}}, 'required': []}}, {'name': 'conversation_response', 'description': \"Takes a user's request which is not handled by ['finding_summarizer', 'investigation_report', 'spl_writer'] and responds with security relevant information and pertinent information already in the chat history. User may ask questions about the investigation data.\", 'examples': ['as9kx', 'why do we exist?', 'Hi how are you?', 'Are you an AI?', 'I see you are a bot', 'explain splunk', 'what can you do with spl', '3 + 3', 'what are your skills?', 'help', 'what is the time of first finding?', 'explain the detections that triggered this finding', 'what is the status of this investigation?', 'give the number of findings in last investigation'], 'parameters': {'type': 'object', 'properties': {'user_input': {'type': 'string', 'description': \"User query with general question or question about value of specific data in the investigation. If user's query doesn't match any other function, pick conversation_response.\"}, 'output': {'type': 'string', 'description': 'Response answering the user question.'}}, 'required': []}}, {'name': 'spl_writer', 'description': \"User input is either natural language or SPL query. Converts a natural language request into an SPL query that can be executed to fulfill the user's intent. User input can be an ask for data analysis, show data insights, creation of data entries, find information from data, data manipulation action, monitor activity, replace data, extract or give entities, show or display or get events/entities, extracting event data from last 24 hours or a time window, or an ask to write code. User input may also ask to modify the previous query or add something to previous query.\", 'examples': ['List available indices', 'your previous SPL did not work', \"Create a record with a value of 'abcd123'\", 'get me all risk entities related to this incident', 'give me another query for this task', 'top 10 IP addresses by count', ' all error messages in the logs from yesterday', 'Show me all the notables for this host', 'average daily ingestion per month', 'previous SPL is wrong', 'show me all notables for this host in the last 60 days', 'Find accounts associated with failed login attempts.', 'check _key field in lookup', 'INFO  CMBucket  Freezing bid', 'list all the hostnames in my deployment', 'who are the users associated with login attempts for linux in past 24 hours', 'list sources that are consuming a lot of GPU RAM', 'search aws access logs last 3 errors', 'show me all the indexes that do not have any logs', 'get risk events in last 24 hours', 'give me users associated with risk event in past 24 hours', 'replace the risk object', 'add user=ax to the last spl query', 'update the search to look for id=123'], 'parameters': {'type': 'object', 'properties': {'intent': {'type': 'string', 'description': \"A natural language description of the user's intent or request. User may ask to modify or add to past SPL query.\"}, 'output': {'type': 'string', 'description': \"The SPL query that corresponds to the user's request.\"}}, 'required': ['intent']}}]\n",
      "\n",
      "    <|eot_id|>\n",
      "    <|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "    make a report<|eot_id|>\n",
      "    <|start_header_id|>assistant<|end_header_id|>\n"
     ]
    }
   ],
   "source": [
    "prompt = generate_llama31_input(orch_prompt, \"make a report\")\n",
    "print(input_data[0])\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Response: system\n",
      "\n",
      "    \n",
      "    You are an expert in picking functions. You are given a question/statement and a set of possible functions.\n",
      "    Based on the question/statement, you will need to make one function to achieve the purpose in question/statement.\n",
      "    You do not provide any explanation, additional text, or answers to user's input as your output. \n",
      "    If the input is correct or incorrect SPL query, pick the function spl_writer. \n",
      "    If the input asks you to create data entries, find from data, do data manipulation or data parsing, show or display data insights, write code, update or add to previous SPL query or asks if the past query was incorrect, pick the function spl_writer. \n",
      "    If the input is gibbrish, pick the function conversation_response. \n",
      "    AS A STRICT RULE, do not answer any of the user questions and STRICTLY only output the function for the user's query from this list of functions:investigation_report, conversation_response, spl_writer, finding_summarizer\"\n",
      "    STRICTLY follow this output format for your response:\n",
      "    Example query: generate report\n",
      "    Your output: investigation_report\n",
      "    Strictly only output the picked function name without any additional text.\n",
      "    Here is a list of functions in JSON format that you can invoke.  \n",
      "    \n",
      "    ### Tool Guidance: \n",
      "\n",
      "\n",
      "You have access to four tools: \n",
      "\n",
      "    [{'name': 'finding_summarizer', 'description': \"Findings Summarizer skill is built for summarizing or in other words explaining the meaning of findings found in an investigation. User query may have the words 'finding' or'summarize' or'summary' in it.\", 'examples': ['summarize this incident', 'write a summary of this incident','summarize findings', 'explain the findings', 'list findings', 'explain the detections that triggered this finding', 'what is the meaning of this incident', 'Can you simplify this (json)'], 'parameters': {'type': 'object', 'properties': {'user_input': {'type':'string', 'description': 'User query asking a summary of the incident. User query may have the word summary or summarize or findings in it.'}, 'output': {'type':'string', 'description': 'A natural language summary of the incident.'}},'required': []}}, {'name': 'investigation_report', 'description': \"Investigation report skill is built to generate a report for the investigation including key information that gives a mid to high level overview of the incident. It gives a chronological timeline of events associated with the incident including the dates, times and descriptions of activity for investigation creation, finding discovery and notes being added by the analyst. User query may have the word'report' in it.\", 'examples': ['write a report', 'Generate a report', 'Give me timeline of events', 'Provide a timeline of key events from analyst notes','report','make an investigation report', 'generate final investigation report for incident'], 'parameters': {'type': 'object', 'properties': {'user_input': {'type':'string', 'description': \"User query asking to generate a report.  User query may have the word'report' in it.\"}, 'output': {'type':'string', 'description': 'A splunk incident report.'}},'required': []}}, {'name': 'conversation_response', 'description': \"Takes a user's request which is not handled by ['finding_summarizer', 'investigation_report','spl_writer'] and responds with security relevant information and pertinent information already in the chat history. User may ask questions about the investigation data.\", 'examples': ['as9kx', 'why do we exist?', 'Hi how are you?', 'Are you an AI?', 'I see you are a bot', 'explain splunk', 'what can you do with spl', '3 + 3', 'what are your skills?', 'help', 'what is the time of first finding?', 'explain the detections that triggered this finding', 'what is the status of this investigation?', 'give the number of findings in last investigation'], 'parameters': {'type': 'object', 'properties': {'user_input': {'type':'string', 'description': \"User query with general question or question about value of specific data in the investigation. If user's query doesn't match any other function, pick conversation_response.\"}, 'output': {'type':'string', 'description': 'Response answering the user question.'}},'required': []}}, {'name':'spl_writer', 'description': \"User input is either natural language or SPL query. Converts a natural language request into an SPL query that can be executed to fulfill the user's intent. User input can be an ask for data analysis, show data insights, creation of data entries, find information from data, data manipulation action, monitor activity, replace data, extract or give entities, show or display or get events/entities, extracting event data from last 24 hours or a time window, or an ask to write code. User input may also ask to modify the previous query or add something to previous query.\", 'examples': ['List available indices', 'your previous SPL did not work', \"Create a record with a value of 'abcd123'\", 'get me all risk entities related to this incident', 'give me another query for this task', 'top 10 IP addresses by count','all error messages in the logs from yesterday', 'Show me all the notables for this host', 'average daily ingestion per month', 'previous SPL is wrong','show me all notables for this host in the last 60 days', 'Find accounts associated with failed login attempts.', 'check _key field in lookup', 'INFO  CMBucket  Freezing bid', 'list all the hostnames in my deployment', 'who are the users associated with login attempts for linux in past 24 hours', 'list sources that are consuming a lot of GPU RAM','search aws access logs last 3 errors','show me all the indexes that do not have any logs', 'get risk events in last 24 hours', 'give me users associated with risk event in past 24 hours','replace the risk object', 'add user=ax to the last spl query', 'update the search to look for id=123'], 'parameters': {'type': 'object', 'properties': {'intent': {'type':'string', 'description': \"A natural language description of the user's intent or request. User may ask to modify or add to past SPL query.\"}, 'output': {'type':'string', 'description': \"The SPL query that corresponds to the user's request.\"}},'required': ['intent']}}]\n",
      "\n",
      "    \n",
      "    user\n",
      "\n",
      "    make a report\n",
      "    assistant\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# import ast\n",
    "# prompt = generate_llama31_input(orch_prompt, str(ast.literal_eval(messages)))\n",
    "\n",
    "#prompt = generate_llama31_input(orch_prompt, messages)\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "output = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=100,  # Adjust max_length as needed\n",
    "    return_dict_in_generate=True,\n",
    "    \n",
    "    output_attentions=True,\n",
    "    output_scores=True\n",
    ")\n",
    "\n",
    "# Decode the generated text\n",
    "response = tokenizer.decode(output.sequences[0], skip_special_tokens=True)\n",
    "print(f\"Generated Response: {response}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lets load the dataframe and get the response for all messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_llama_out_wattention(input1):\n",
    "    prompt = generate_llama31_input(orch_prompt, input1)\n",
    "\n",
    "    #prompt = generate_llama31_input(orch_prompt, messages)\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    output = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=100,  # Adjust max_length as needed\n",
    "        return_dict_in_generate=True,\n",
    "\n",
    "        output_attentions=True,\n",
    "        output_scores=True\n",
    "    )\n",
    "\n",
    "    # Decode the generated text\n",
    "    response = tokenizer.decode(output.sequences[0], skip_special_tokens=True)\n",
    "    \n",
    "    \n",
    "#     msgs = ast.literal_eval(msgs)\n",
    "#     new_msg = f'''{msgs} \\n \\n '''\n",
    "#     llama_prompt = generate_llama31_input(orch_prompt, new_msg)\n",
    "    \n",
    "#     inputs = tokenizer.encode(llama_prompt, return_tensors=\"pt\")\n",
    "\n",
    "#     output_sequences = model.generate(input_ids=inputs, max_new_tokens=8, output_attentions=True, output_hidden_states=True, return_dict_in_generate=True)\n",
    "\n",
    "#     response = tokenizer.decode(output_sequences.sequences[0], skip_special_tokens=True)\n",
    "    \n",
    "    return response, output.attentions, output.hidden_states\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import arange, cartesian_prod, max as torch_max, tensor, Tensor, zeros\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_subarray( large_list, small_list):\n",
    "        large_len = len(large_list)\n",
    "        small_len = len(small_list)\n",
    "        \n",
    "        for i in range(large_len - small_len + 1):\n",
    "            if large_list[i:i+small_len] == small_list:\n",
    "                return i\n",
    "        \n",
    "        return -1\n",
    "\n",
    "    \n",
    "def search_for_target_sequence(tokenizer,target_text, prompt, input_ids):\n",
    "        '''\n",
    "        grab the template part from input\n",
    "        '''\n",
    "        tokenized_text = tokenizer.tokenize(target_text)\n",
    "        reference_tokens = tokenizer.tokenize(prompt)\n",
    "        assert len(reference_tokens) == input_ids.shape[1] -1 # this model will automatically add one token at the beginning\n",
    "        assert set(tokenized_text).issubset(set(reference_tokens))\n",
    "        last_index = find_subarray(reference_tokens, tokenized_text)\n",
    "        assert last_index != -1, \"token not found\"\n",
    "        # last_index = reference_tokens.index(tokenized_text[-1]) # find the position of last token in target text\n",
    "        return last_index + len(tokenized_text) - 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[128000, 128000, 128006,  ..., 128006,  78191, 128007]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]])}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's get attention weights with following approaches\n",
    "# 1. With sys_prompt + identifier + user_msg\n",
    "# 2. With sys_prompt + chat_history + identifier + user_msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n    \\n    You are an expert in picking functions. You are given a question/statement and a set of possible functions.\\n    Based on the question/statement, you will need to make one function to achieve the purpose in question/statement.\\n    You do not provide any explanation, additional text, or answers to user\\'s input as your output. \\n    If the input is correct or incorrect SPL query, pick the function spl_writer. \\n    If the input asks you to create data entries, find from data, do data manipulation or data parsing, show or display data insights, write code, update or add to previous SPL query or asks if the past query was incorrect, pick the function spl_writer. \\n    If the input is gibbrish, pick the function conversation_response. \\n    AS A STRICT RULE, do not answer any of the user questions and STRICTLY only output the function for the user\\'s query from this list of functions:investigation_report, conversation_response, spl_writer, finding_summarizer\"\\n    STRICTLY follow this output format for your response:\\n    Example query: generate report\\n    Your output: investigation_report\\n    Strictly only output the picked function name without any additional text.\\n    Here is a list of functions in JSON format that you can invoke.  \\n    \\n    ### Tool Guidance: \\n\\n\\nYou have access to four tools: \\n\\n    [{\\'name\\': \\'finding_summarizer\\', \\'description\\': \"Findings Summarizer skill is built for summarizing or in other words explaining the meaning of findings found in an investigation. User query may have the words \\'finding\\' or \\'summarize\\' or \\'summary\\' in it.\", \\'examples\\': [\\'summarize this incident\\', \\'write a summary of this incident\\', \\'summarize findings\\', \\'explain the findings\\', \\'list findings\\', \\'explain the detections that triggered this finding\\', \\'what is the meaning of this incident\\', \\'Can you simplify this (json)\\'], \\'parameters\\': {\\'type\\': \\'object\\', \\'properties\\': {\\'user_input\\': {\\'type\\': \\'string\\', \\'description\\': \\'User query asking a summary of the incident. User query may have the word summary or summarize or findings in it.\\'}, \\'output\\': {\\'type\\': \\'string\\', \\'description\\': \\'A natural language summary of the incident.\\'}}, \\'required\\': []}}, {\\'name\\': \\'investigation_report\\', \\'description\\': \"Investigation report skill is built to generate a report for the investigation including key information that gives a mid to high level overview of the incident. It gives a chronological timeline of events associated with the incident including the dates, times and descriptions of activity for investigation creation, finding discovery and notes being added by the analyst. User query may have the word \\'report\\' in it.\", \\'examples\\': [\\'write a report\\', \\'Generate a report\\', \\'Give me timeline of events\\', \\'Provide a timeline of key events from analyst notes\\', \\'report\\', \\'make an investigation report\\', \\'generate final investigation report for incident\\'], \\'parameters\\': {\\'type\\': \\'object\\', \\'properties\\': {\\'user_input\\': {\\'type\\': \\'string\\', \\'description\\': \"User query asking to generate a report.  User query may have the word \\'report\\' in it.\"}, \\'output\\': {\\'type\\': \\'string\\', \\'description\\': \\'A splunk incident report.\\'}}, \\'required\\': []}}, {\\'name\\': \\'conversation_response\\', \\'description\\': \"Takes a user\\'s request which is not handled by [\\'finding_summarizer\\', \\'investigation_report\\', \\'spl_writer\\'] and responds with security relevant information and pertinent information already in the chat history. User may ask questions about the investigation data.\", \\'examples\\': [\\'as9kx\\', \\'why do we exist?\\', \\'Hi how are you?\\', \\'Are you an AI?\\', \\'I see you are a bot\\', \\'explain splunk\\', \\'what can you do with spl\\', \\'3 + 3\\', \\'what are your skills?\\', \\'help\\', \\'what is the time of first finding?\\', \\'explain the detections that triggered this finding\\', \\'what is the status of this investigation?\\', \\'give the number of findings in last investigation\\'], \\'parameters\\': {\\'type\\': \\'object\\', \\'properties\\': {\\'user_input\\': {\\'type\\': \\'string\\', \\'description\\': \"User query with general question or question about value of specific data in the investigation. If user\\'s query doesn\\'t match any other function, pick conversation_response.\"}, \\'output\\': {\\'type\\': \\'string\\', \\'description\\': \\'Response answering the user question.\\'}}, \\'required\\': []}}, {\\'name\\': \\'spl_writer\\', \\'description\\': \"User input is either natural language or SPL query. Converts a natural language request into an SPL query that can be executed to fulfill the user\\'s intent. User input can be an ask for data analysis, show data insights, creation of data entries, find information from data, data manipulation action, monitor activity, replace data, extract or give entities, show or display or get events/entities, extracting event data from last 24 hours or a time window, or an ask to write code. User input may also ask to modify the previous query or add something to previous query.\", \\'examples\\': [\\'List available indices\\', \\'your previous SPL did not work\\', \"Create a record with a value of \\'abcd123\\'\", \\'get me all risk entities related to this incident\\', \\'give me another query for this task\\', \\'top 10 IP addresses by count\\', \\' all error messages in the logs from yesterday\\', \\'Show me all the notables for this host\\', \\'average daily ingestion per month\\', \\'previous SPL is wrong\\', \\'show me all notables for this host in the last 60 days\\', \\'Find accounts associated with failed login attempts.\\', \\'check _key field in lookup\\', \\'INFO  CMBucket  Freezing bid\\', \\'list all the hostnames in my deployment\\', \\'who are the users associated with login attempts for linux in past 24 hours\\', \\'list sources that are consuming a lot of GPU RAM\\', \\'search aws access logs last 3 errors\\', \\'show me all the indexes that do not have any logs\\', \\'get risk events in last 24 hours\\', \\'give me users associated with risk event in past 24 hours\\', \\'replace the risk object\\', \\'add user=ax to the last spl query\\', \\'update the search to look for id=123\\'], \\'parameters\\': {\\'type\\': \\'object\\', \\'properties\\': {\\'intent\\': {\\'type\\': \\'string\\', \\'description\\': \"A natural language description of the user\\'s intent or request. User may ask to modify or add to past SPL query.\"}, \\'output\\': {\\'type\\': \\'string\\', \\'description\\': \"The SPL query that corresponds to the user\\'s request.\"}}, \\'required\\': [\\'intent\\']}}]\\n\\n    <|eot_id|>\\n    <|start_header_id|>user<|end_header_id|>\\n\\n    make a report<|eot_id|>\\n    <|start_header_id|>assistant<|end_header_id|>'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = generate_llama31_input(orch_prompt, \"make a report\")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Response: system\n",
      "\n",
      "    \n",
      "    You are an expert in picking functions. You are given a question/statement and a set of possible functions.\n",
      "    Based on the question/statement, you will need to make one function to achieve the purpose in question/statement.\n",
      "    You do not provide any explanation, additional text, or answers to user's input as your output. \n",
      "    If the input is correct or incorrect SPL query, pick the function spl_writer. \n",
      "    If the input asks you to create data entries, find from data, do data manipulation or data parsing, show or display data insights, write code, update or add to previous SPL query or asks if the past query was incorrect, pick the function spl_writer. \n",
      "    If the input is gibbrish, pick the function conversation_response. \n",
      "    AS A STRICT RULE, do not answer any of the user questions and STRICTLY only output the function for the user's query from this list of functions:investigation_report, conversation_response, spl_writer, finding_summarizer\"\n",
      "    STRICTLY follow this output format for your response:\n",
      "    Example query: generate report\n",
      "    Your output: investigation_report\n",
      "    Strictly only output the picked function name without any additional text.\n",
      "    Here is a list of functions in JSON format that you can invoke.  \n",
      "    \n",
      "    ### Tool Guidance: \n",
      "\n",
      "\n",
      "You have access to four tools: \n",
      "\n",
      "    [{'name': 'finding_summarizer', 'description': \"Findings Summarizer skill is built for summarizing or in other words explaining the meaning of findings found in an investigation. User query may have the words 'finding' or'summarize' or'summary' in it.\", 'examples': ['summarize this incident', 'write a summary of this incident','summarize findings', 'explain the findings', 'list findings', 'explain the detections that triggered this finding', 'what is the meaning of this incident', 'Can you simplify this (json)'], 'parameters': {'type': 'object', 'properties': {'user_input': {'type':'string', 'description': 'User query asking a summary of the incident. User query may have the word summary or summarize or findings in it.'}, 'output': {'type':'string', 'description': 'A natural language summary of the incident.'}},'required': []}}, {'name': 'investigation_report', 'description': \"Investigation report skill is built to generate a report for the investigation including key information that gives a mid to high level overview of the incident. It gives a chronological timeline of events associated with the incident including the dates, times and descriptions of activity for investigation creation, finding discovery and notes being added by the analyst. User query may have the word'report' in it.\", 'examples': ['write a report', 'Generate a report', 'Give me timeline of events', 'Provide a timeline of key events from analyst notes','report','make an investigation report', 'generate final investigation report for incident'], 'parameters': {'type': 'object', 'properties': {'user_input': {'type':'string', 'description': \"User query asking to generate a report.  User query may have the word'report' in it.\"}, 'output': {'type':'string', 'description': 'A splunk incident report.'}},'required': []}}, {'name': 'conversation_response', 'description': \"Takes a user's request which is not handled by ['finding_summarizer', 'investigation_report','spl_writer'] and responds with security relevant information and pertinent information already in the chat history. User may ask questions about the investigation data.\", 'examples': ['as9kx', 'why do we exist?', 'Hi how are you?', 'Are you an AI?', 'I see you are a bot', 'explain splunk', 'what can you do with spl', '3 + 3', 'what are your skills?', 'help', 'what is the time of first finding?', 'explain the detections that triggered this finding', 'what is the status of this investigation?', 'give the number of findings in last investigation'], 'parameters': {'type': 'object', 'properties': {'user_input': {'type':'string', 'description': \"User query with general question or question about value of specific data in the investigation. If user's query doesn't match any other function, pick conversation_response.\"}, 'output': {'type':'string', 'description': 'Response answering the user question.'}},'required': []}}, {'name':'spl_writer', 'description': \"User input is either natural language or SPL query. Converts a natural language request into an SPL query that can be executed to fulfill the user's intent. User input can be an ask for data analysis, show data insights, creation of data entries, find information from data, data manipulation action, monitor activity, replace data, extract or give entities, show or display or get events/entities, extracting event data from last 24 hours or a time window, or an ask to write code. User input may also ask to modify the previous query or add something to previous query.\", 'examples': ['List available indices', 'your previous SPL did not work', \"Create a record with a value of 'abcd123'\", 'get me all risk entities related to this incident', 'give me another query for this task', 'top 10 IP addresses by count','all error messages in the logs from yesterday', 'Show me all the notables for this host', 'average daily ingestion per month', 'previous SPL is wrong','show me all notables for this host in the last 60 days', 'Find accounts associated with failed login attempts.', 'check _key field in lookup', 'INFO  CMBucket  Freezing bid', 'list all the hostnames in my deployment', 'who are the users associated with login attempts for linux in past 24 hours', 'list sources that are consuming a lot of GPU RAM','search aws access logs last 3 errors','show me all the indexes that do not have any logs', 'get risk events in last 24 hours', 'give me users associated with risk event in past 24 hours','replace the risk object', 'add user=ax to the last spl query', 'update the search to look for id=123'], 'parameters': {'type': 'object', 'properties': {'intent': {'type':'string', 'description': \"A natural language description of the user's intent or request. User may ask to modify or add to past SPL query.\"}, 'output': {'type':'string', 'description': \"The SPL query that corresponds to the user's request.\"}},'required': ['intent']}}]\n",
      "\n",
      "    \n",
      "    user\n",
      "\n",
      "    make a report\n",
      "    assistant\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "output = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=1,  # Adjust max_length as needed\n",
    "    return_dict_in_generate=True,\n",
    "    \n",
    "    output_attentions=True,\n",
    "    output_scores=True\n",
    ")\n",
    "\n",
    "# Decode the generated text\n",
    "response = tokenizer.decode(output.sequences[0], skip_special_tokens=True)\n",
    "print(f\"Generated Response: {response}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[128000, 128000, 128006,  ..., 128006,  78191, 128007]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]])}\n",
      ":\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "print(inputs)\n",
    "set_a_start_ids = search_for_target_sequence(tokenizer,\"You have access to four tools\", prompt, inputs[\"input_ids\"])+2\n",
    "\n",
    "\n",
    "input_ids= tokenizer(prompt, return_tensors=\"pt\")\n",
    "tmp1=np.array(np.array(input_ids[\"input_ids\"][0].cpu())[set_a_start_ids])\n",
    "tmp1\n",
    "print(tokenizer.decode(tmp1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:284\u001b[0m, in \u001b[0;36mBatchEncoding.__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 284\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "\u001b[0;31mKeyError\u001b[0m: 'shape'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[99], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m set_a_start_ids \u001b[38;5;241m=\u001b[39m \u001b[43msearch_for_target_sequence\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mYou have access to four tools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m2\u001b[39m\n",
      "Cell \u001b[0;32mIn[78], line 18\u001b[0m, in \u001b[0;36msearch_for_target_sequence\u001b[0;34m(tokenizer, target_text, prompt, input_ids)\u001b[0m\n\u001b[1;32m     16\u001b[0m tokenized_text \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(target_text)\n\u001b[1;32m     17\u001b[0m reference_tokens \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(prompt)\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(reference_tokens) \u001b[38;5;241m==\u001b[39m \u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;66;03m# this model will automatically add one token at the beginning\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mset\u001b[39m(tokenized_text)\u001b[38;5;241m.\u001b[39missubset(\u001b[38;5;28mset\u001b[39m(reference_tokens))\n\u001b[1;32m     20\u001b[0m last_index \u001b[38;5;241m=\u001b[39m find_subarray(reference_tokens, tokenized_text)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:286\u001b[0m, in \u001b[0;36mBatchEncoding.__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[item]\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[0;32m--> 286\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "set_a_start_ids = search_for_target_sequence(tokenizer,\"You have access to four tools\", prompt, inputs)+2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "invalid index to scalar variable.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[95], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m275\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mIndexError\u001b[0m: invalid index to scalar variable."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "input_ids= tokenizer(prompt, return_tensors=\"pt\")\n",
    "tmp1=np.array(np.array(input_ids[0][0].cpu())[275])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#search_for_target_sequence(tokenizer,\"investigation_report, conversation_response, spl_writer, findings_summarizer\", prompt, inputs['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1389"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#to identify ending sequence\n",
    "search_for_target_sequence(tokenizer,\"<|eot_id|>\\n    <|start_header_id|>user<|end_header_id|>\",prompt, inputs['input_ids'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1205"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_for_target_sequence(tokenizer,\"Who is the assignee for this case\",prompt, inputs['input_ids'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1226"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_for_target_sequence(tokenizer,\"<|start_header_id|>assistant<|end_header_id|>\",prompt, inputs['input_ids'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#search_for_target_sequence(tokenizer,\"<|eot_id|>\\n    <|start_header_id|>assistant<|end_header_id|>\",prompt, inputs['input_ids'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#search_for_target_sequence(tokenizer,\"<|eot_id|>\\n    <|start_header_id|>assistant<|end_header_id|>\", prompt, inputs['input_ids'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#search_for_target_sequence(tokenizer,\"List all\",prompt, inputs['input_ids'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#search_for_target_sequence(tokenizer,df.latest_msg[49], prompt, inputs['input_ids']) - len(tokenizer.tokenize(df.latest_msg[49]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llama_extract_latent_features(prompt=prompt, max_tokens=1):\n",
    "    #prompt = generate_llama31_input(orch_prompt, str(ast.literal_eval(messages)))\n",
    "\n",
    "    #prompt = generate_llama31_input(orch_prompt, messages)\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    output = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_tokens,  # Adjust max_length as needed\n",
    "        return_dict_in_generate=True,\n",
    "        output_hidden_states=True,\n",
    "        output_attentions=True,\n",
    "        #output_scores=True\n",
    "    )\n",
    "\n",
    "    # Decode the generated text\n",
    "    #response = tokenizer.decode(output.sequences[0], skip_special_tokens=True)\n",
    "    \n",
    "    return inputs['input_ids'], output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "attention_graph = extract_llm_graph(model, prompt, max_tokens=1,latest_message = lat_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "input_ids, outputs  = llama_extract_latent_features(prompt=prompt, max_tokens=8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_llm = tokenizer\n",
    "set_a_start_ids = search_for_target_sequence(tokenizer_llm,\"You have access to four tools\", prompt, input_ids)+2\n",
    "set_a_end_ids = search_for_target_sequence(tokenizer_llm,\"<|eot_id|>\\n    <|start_header_id|>user<|end_header_id|>\", prompt, input_ids)-10\n",
    "    \n",
    "set_b_end_ids = search_for_target_sequence(tokenizer_llm,\"<|start_header_id|>assistant<|end_header_id|>\", prompt, input_ids)\n",
    "    \n",
    "attention_graph=extract_latent_feature_graph(input_ids=input_ids, outputs=outputs, position_index_a_endpoints=(set_a_start_ids, set_a_end_ids), position_index_b_endpoints=(set_a_end_ids+2, set_b_end_ids-4))\n",
    "\n",
    "        \n",
    "       \n",
    "       \n",
    "       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[1119, 4096], edge_index=[2, 17648], edge_attr=[17648, 1], ids=[1119], node_types=[1119])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/pytorch/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:818: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_attentions` is. When `return_dict_in_generate` is not `True`, `output_attentions` is ignored.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f47039c6dc414cbba5fd0f2b8dc6c5a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/174 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "/opt/conda/envs/pytorch/lib/python3.11/site-packages/transformers/generation/utils.py:2137: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  1%|▏                                          | 1/174 [00:05<15:25,  5.35s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  1%|▍                                          | 2/174 [00:09<13:53,  4.85s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  2%|▋                                          | 3/174 [00:15<14:50,  5.21s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  2%|▉                                          | 4/174 [00:21<15:14,  5.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  3%|█▏                                         | 5/174 [00:26<15:27,  5.49s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  3%|█▍                                         | 6/174 [00:32<15:30,  5.54s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  4%|█▋                                         | 7/174 [00:38<15:32,  5.58s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  5%|█▉                                         | 8/174 [00:42<14:29,  5.24s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  5%|██▏                                        | 9/174 [00:48<14:45,  5.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  6%|██▍                                       | 10/174 [00:53<14:54,  5.45s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  6%|██▋                                       | 11/174 [00:59<14:59,  5.52s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  7%|██▉                                       | 12/174 [01:04<14:24,  5.34s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  7%|███▏                                      | 13/174 [01:10<14:36,  5.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  8%|███▍                                      | 14/174 [01:15<14:40,  5.50s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  9%|███▌                                      | 15/174 [01:21<14:43,  5.56s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  9%|███▊                                      | 16/174 [01:26<14:08,  5.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 10%|████                                      | 17/174 [01:32<14:18,  5.47s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 10%|████▎                                     | 18/174 [01:36<13:40,  5.26s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 11%|████▌                                     | 19/174 [01:42<13:55,  5.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 11%|████▊                                     | 20/174 [01:48<14:02,  5.47s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 12%|█████                                     | 21/174 [01:53<14:06,  5.53s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 13%|█████▎                                    | 22/174 [01:59<14:07,  5.58s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 13%|█████▌                                    | 23/174 [02:05<14:08,  5.62s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 14%|█████▊                                    | 24/174 [02:11<14:05,  5.64s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 14%|██████                                    | 25/174 [02:15<13:09,  5.30s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 15%|██████▎                                   | 26/174 [02:21<13:21,  5.42s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 16%|██████▌                                   | 27/174 [02:26<13:28,  5.50s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 16%|██████▊                                   | 28/174 [02:32<13:30,  5.55s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 17%|███████                                   | 29/174 [02:39<14:28,  5.99s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 17%|███████▏                                  | 30/174 [02:46<15:07,  6.30s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 18%|███████▍                                  | 31/174 [02:52<14:47,  6.21s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 18%|███████▋                                  | 32/174 [02:59<15:22,  6.50s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 19%|███████▉                                  | 33/174 [03:07<15:49,  6.74s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 20%|████████▏                                 | 34/174 [03:14<16:03,  6.88s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 20%|████████▍                                 | 35/174 [03:21<16:09,  6.98s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 21%|████████▋                                 | 36/174 [03:27<15:29,  6.74s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 21%|████████▉                                 | 37/174 [03:33<14:40,  6.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 22%|█████████▏                                | 38/174 [03:39<14:00,  6.18s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 22%|█████████▍                                | 39/174 [03:44<13:30,  6.01s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 23%|█████████▋                                | 40/174 [03:50<13:08,  5.88s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 24%|█████████▉                                | 41/174 [04:04<18:32,  8.36s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 24%|██████████▏                               | 42/174 [04:10<16:51,  7.67s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 25%|██████████▍                               | 43/174 [04:16<15:25,  7.07s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 25%|██████████▌                               | 44/174 [04:21<14:11,  6.55s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 26%|██████████▊                               | 45/174 [04:27<13:29,  6.27s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 26%|███████████                               | 46/174 [04:32<12:56,  6.06s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 27%|███████████▎                              | 47/174 [04:38<12:36,  5.95s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 28%|███████████▌                              | 48/174 [04:43<12:10,  5.80s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 28%|███████████▊                              | 49/174 [04:49<12:01,  5.77s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 29%|████████████                              | 50/174 [04:54<11:46,  5.70s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 29%|████████████▎                             | 51/174 [04:59<10:51,  5.30s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 30%|████████████▌                             | 52/174 [05:03<10:06,  4.97s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 30%|████████████▊                             | 53/174 [05:08<09:49,  4.87s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 31%|█████████████                             | 54/174 [05:13<10:06,  5.05s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 32%|█████████████▎                            | 55/174 [05:18<09:46,  4.93s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 32%|█████████████▌                            | 56/174 [05:23<09:59,  5.08s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 33%|█████████████▊                            | 57/174 [05:29<10:14,  5.25s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 33%|██████████████                            | 58/174 [05:34<10:19,  5.34s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 34%|██████████████▏                           | 59/174 [05:39<09:45,  5.09s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 34%|██████████████▍                           | 60/174 [05:43<09:14,  4.86s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 35%|██████████████▋                           | 61/174 [05:49<09:36,  5.10s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 36%|██████████████▉                           | 62/174 [05:54<09:46,  5.23s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 36%|███████████████▏                          | 63/174 [06:00<09:55,  5.36s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 37%|███████████████▍                          | 64/174 [06:06<09:54,  5.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 37%|███████████████▋                          | 65/174 [06:11<10:01,  5.52s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 38%|███████████████▉                          | 66/174 [06:17<10:00,  5.56s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 39%|████████████████▏                         | 67/174 [06:23<09:58,  5.60s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 39%|████████████████▍                         | 68/174 [06:28<09:35,  5.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 40%|████████████████▋                         | 69/174 [06:34<09:38,  5.51s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████████████████▉                         | 70/174 [06:38<09:00,  5.20s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 41%|█████████████████▏                        | 71/174 [06:42<08:32,  4.98s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 41%|█████████████████▍                        | 72/174 [06:48<08:44,  5.14s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 42%|█████████████████▌                        | 73/174 [06:53<08:33,  5.08s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 43%|█████████████████▊                        | 74/174 [06:57<08:10,  4.90s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 43%|██████████████████                        | 75/174 [07:03<08:29,  5.15s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 44%|██████████████████▎                       | 76/174 [07:09<08:40,  5.32s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 44%|██████████████████▌                       | 77/174 [07:13<08:10,  5.06s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 45%|██████████████████▊                       | 78/174 [07:19<08:24,  5.25s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 45%|███████████████████                       | 79/174 [07:25<08:31,  5.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 46%|███████████████████▎                      | 80/174 [07:30<08:34,  5.47s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 47%|███████████████████▌                      | 81/174 [07:36<08:34,  5.54s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 47%|███████████████████▊                      | 82/174 [07:42<08:33,  5.58s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 48%|████████████████████                      | 83/174 [07:47<08:30,  5.61s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 48%|████████████████████▎                     | 84/174 [07:53<08:25,  5.62s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 49%|████████████████████▌                     | 85/174 [07:58<07:51,  5.30s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 49%|████████████████████▊                     | 86/174 [08:03<07:55,  5.41s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 50%|█████████████████████                     | 87/174 [08:08<07:27,  5.14s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 51%|█████████████████████▏                    | 88/174 [08:13<07:17,  5.09s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 51%|█████████████████████▍                    | 89/174 [08:18<07:27,  5.26s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 52%|█████████████████████▋                    | 90/174 [08:24<07:32,  5.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 52%|█████████████████████▉                    | 91/174 [08:29<07:17,  5.27s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 53%|██████████████████████▏                   | 92/174 [08:35<07:24,  5.42s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 53%|██████████████████████▍                   | 93/174 [08:39<06:57,  5.15s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 54%|██████████████████████▋                   | 94/174 [08:45<07:04,  5.30s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 55%|██████████████████████▉                   | 95/174 [08:50<06:47,  5.16s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 55%|███████████████████████▏                  | 96/174 [08:56<06:53,  5.31s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 56%|███████████████████████▍                  | 97/174 [09:01<06:56,  5.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 56%|███████████████████████▋                  | 98/174 [09:06<06:36,  5.21s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 57%|███████████████████████▉                  | 99/174 [09:10<06:14,  4.99s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 57%|███████████████████████▌                 | 100/174 [09:16<06:23,  5.18s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 58%|███████████████████████▊                 | 101/174 [09:21<06:04,  5.00s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 59%|████████████████████████                 | 102/174 [09:25<05:49,  4.86s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 59%|████████████████████████▎                | 103/174 [09:31<06:01,  5.09s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 60%|████████████████████████▌                | 104/174 [09:36<06:08,  5.26s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 60%|████████████████████████▋                | 105/174 [09:42<06:12,  5.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 61%|████████████████████████▉                | 106/174 [09:47<05:48,  5.13s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 61%|█████████████████████████▏               | 107/174 [09:52<05:53,  5.28s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 62%|█████████████████████████▍               | 108/174 [09:58<05:55,  5.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 63%|█████████████████████████▋               | 109/174 [10:04<05:57,  5.49s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 63%|█████████████████████████▉               | 110/174 [10:08<05:32,  5.20s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 64%|██████████████████████████▏              | 111/174 [10:13<05:14,  4.99s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 64%|██████████████████████████▍              | 112/174 [10:18<05:22,  5.19s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 65%|██████████████████████████▋              | 113/174 [10:23<05:05,  5.00s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 66%|██████████████████████████▊              | 114/174 [10:29<05:11,  5.20s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 66%|███████████████████████████              | 115/174 [10:34<05:15,  5.34s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 67%|███████████████████████████▎             | 116/174 [10:40<05:15,  5.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 67%|███████████████████████████▌             | 117/174 [10:45<05:04,  5.35s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 68%|███████████████████████████▊             | 118/174 [10:51<05:04,  5.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 68%|████████████████████████████             | 119/174 [10:56<05:02,  5.50s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 69%|████████████████████████████▎            | 120/174 [11:01<04:48,  5.35s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 70%|████████████████████████████▌            | 121/174 [11:07<04:48,  5.45s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 70%|████████████████████████████▋            | 122/174 [11:13<04:46,  5.51s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 71%|████████████████████████████▉            | 123/174 [11:17<04:27,  5.24s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 71%|█████████████████████████████▏           | 124/174 [11:22<04:10,  5.01s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 72%|█████████████████████████████▍           | 125/174 [11:27<04:16,  5.22s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 72%|█████████████████████████████▋           | 126/174 [11:33<04:19,  5.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 73%|█████████████████████████████▉           | 127/174 [11:39<04:17,  5.48s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 74%|██████████████████████████████▏          | 128/174 [11:45<04:14,  5.53s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 74%|██████████████████████████████▍          | 129/174 [11:50<04:11,  5.58s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 75%|██████████████████████████████▋          | 130/174 [11:56<04:08,  5.66s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 75%|██████████████████████████████▊          | 131/174 [12:02<04:05,  5.70s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 76%|███████████████████████████████          | 132/174 [12:08<03:59,  5.69s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 76%|███████████████████████████████▎         | 133/174 [12:13<03:54,  5.73s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 77%|███████████████████████████████▌         | 134/174 [12:19<03:48,  5.71s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 78%|███████████████████████████████▊         | 135/174 [12:25<03:42,  5.71s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 78%|████████████████████████████████         | 136/174 [12:31<03:37,  5.71s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 79%|████████████████████████████████▎        | 137/174 [12:35<03:17,  5.35s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 79%|████████████████████████████████▌        | 138/174 [12:39<03:03,  5.09s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 80%|████████████████████████████████▊        | 139/174 [12:45<03:03,  5.25s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 80%|████████████████████████████████▉        | 140/174 [12:51<03:03,  5.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 81%|█████████████████████████████████▏       | 141/174 [12:56<03:00,  5.47s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|█████████████████████████████████▍       | 142/174 [13:02<02:57,  5.54s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 82%|█████████████████████████████████▋       | 143/174 [13:08<02:53,  5.58s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 83%|█████████████████████████████████▉       | 144/174 [13:14<02:48,  5.61s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 83%|██████████████████████████████████▏      | 145/174 [13:19<02:39,  5.50s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 84%|██████████████████████████████████▍      | 146/174 [13:24<02:35,  5.55s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 84%|██████████████████████████████████▋      | 147/174 [13:30<02:30,  5.59s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 85%|██████████████████████████████████▊      | 148/174 [13:36<02:25,  5.61s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 86%|███████████████████████████████████      | 149/174 [13:41<02:20,  5.64s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 86%|███████████████████████████████████▎     | 150/174 [13:47<02:15,  5.65s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 87%|███████████████████████████████████▌     | 151/174 [13:53<02:10,  5.65s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 87%|███████████████████████████████████▊     | 152/174 [13:58<02:03,  5.59s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 88%|████████████████████████████████████     | 153/174 [14:03<01:54,  5.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 89%|████████████████████████████████████▎    | 154/174 [14:08<01:46,  5.34s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 89%|████████████████████████████████████▌    | 155/174 [14:14<01:43,  5.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 90%|████████████████████████████████████▊    | 156/174 [14:20<01:39,  5.51s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 90%|████████████████████████████████████▉    | 157/174 [14:25<01:34,  5.56s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 91%|█████████████████████████████████████▏   | 158/174 [14:31<01:29,  5.60s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 91%|█████████████████████████████████████▍   | 159/174 [14:37<01:24,  5.62s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 92%|█████████████████████████████████████▋   | 160/174 [14:41<01:14,  5.29s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 93%|█████████████████████████████████████▉   | 161/174 [14:47<01:10,  5.41s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 93%|██████████████████████████████████████▏  | 162/174 [14:53<01:05,  5.48s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 94%|██████████████████████████████████████▍  | 163/174 [14:58<01:00,  5.54s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 94%|██████████████████████████████████████▋  | 164/174 [15:04<00:55,  5.58s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 95%|██████████████████████████████████████▉  | 165/174 [15:09<00:47,  5.27s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 95%|███████████████████████████████████████  | 166/174 [15:14<00:43,  5.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 96%|███████████████████████████████████████▎ | 167/174 [15:20<00:38,  5.48s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 97%|███████████████████████████████████████▌ | 168/174 [15:26<00:33,  5.55s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 97%|███████████████████████████████████████▊ | 169/174 [15:31<00:27,  5.59s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 98%|████████████████████████████████████████ | 170/174 [15:36<00:21,  5.41s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 98%|████████████████████████████████████████▎| 171/174 [15:41<00:15,  5.31s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 99%|████████████████████████████████████████▌| 172/174 [15:47<00:10,  5.41s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 99%|████████████████████████████████████████▊| 173/174 [15:53<00:05,  5.51s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "100%|█████████████████████████████████████████| 174/174 [15:58<00:00,  5.51s/it]\n"
     ]
    }
   ],
   "source": [
    "# def extract_arguments() -> ArgumentParser:\n",
    "#     parser = ArgumentParser()\n",
    "#     parser.add_argument(\"--input_file\", type=str, default=\"saia_intent.csv\")\n",
    "#     parser.add_argument(\"--output_directory\", type=str, default=\"data/grading/orchestration/hybrid_orchestration/spl\")\n",
    "#     parser.add_argument(\"--function_template_file_path\", type=str, default=\"/home/azureuser/lbetthauser/data/hybrid_orchestration/spl/function_metadata.json\")\n",
    "#     parser.add_argument(\"--limit\", type=int, default=-1)\n",
    "    \n",
    "#     return parser.parse_args()\n",
    "# input_file=\"orchestrator_data/orch_data_expt.csv\"\n",
    "output_directory='' \n",
    "#function_template_file_path=\"function_metadata.json\"\n",
    "limit=-1\n",
    "\n",
    "# def load_data(file_path):\n",
    "#     \"\"\"\n",
    "#     Loads data from the specified file path.\n",
    "#     \"\"\"\n",
    "#     return load_csv_file(file_path)\n",
    "\n",
    "\n",
    "# # def load_function_templates(tools):\n",
    "# #     return {\n",
    "# #         template[\"name\"]: template\n",
    "# #     for template in tools}\n",
    "# def load_function_templates(tools):\n",
    "#     return {\n",
    "#         template[\"name\"]: template\n",
    "#     for template in tools}\n",
    "\n",
    "\n",
    "def extract_llm_graph(\n",
    "    model,\n",
    "    prompt:str,\n",
    "    max_tokens:int=1,\n",
    "    latest_message:str=''\n",
    "):\n",
    "    # compute decoder attention\n",
    "    \n",
    "    input_ids, outputs  = llama_extract_latent_features(prompt=prompt, max_tokens=max_tokens)\n",
    "    #specify token ids for relevant subportion of the prompt\n",
    "    #template_start_ids = search_for_target_sequence(tokenizer_llm,\"You have access to three security tools\", prompt, input_ids)\n",
    "    tokenizer_llm = tokenizer\n",
    "    \n",
    "    set_a_start_ids = search_for_target_sequence(tokenizer_llm,\"You have access to four tools\", prompt, input_ids)+2\n",
    "    set_a_end_ids = search_for_target_sequence(tokenizer_llm,\"<|eot_id|>\\n    <|start_header_id|>user<|end_header_id|>\", prompt, input_ids)-10\n",
    "    \n",
    "    set_b_end_ids = search_for_target_sequence(tokenizer_llm,\"<|start_header_id|>assistant<|end_header_id|>\", prompt, input_ids)\n",
    "    \n",
    "    \n",
    "   \n",
    "    # construct attention graph\n",
    "    return extract_latent_feature_graph(\n",
    "        input_ids=input_ids,\n",
    "        outputs=outputs,\n",
    "        position_index_a_endpoints=(set_a_start_ids, set_a_end_ids),\n",
    "        position_index_b_endpoints=(set_a_end_ids+2, set_b_end_ids-4)\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "def extract_llm_graphs(\n",
    "    input_file:str,\n",
    "    output_directory:str, \n",
    "    #function_template_file_path:str,\n",
    "    limit:int=200\n",
    ") -> None:\n",
    "    data = df\n",
    "    labels = data['label']\n",
    "    inputs = data['modified_msg']\n",
    "    latest_msgs = data['latest_msg']\n",
    "    responses = []\n",
    "    graph_latency = []\n",
    "    response_latency = []\n",
    "    label_mapping = {value: idx for idx, value in enumerate(list(set(labels)))}\n",
    "    \n",
    "    attention_graphs = []\n",
    "    \n",
    "    #function_templates = load_function_templates(function_template_file_path)\n",
    "    #function_templates = tools\n",
    "\n",
    "    # reference model\n",
    "    #model = llm()\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, output_attentions=True, torch_dtype=torch.float16, device_map=\"auto\",\\\n",
    "                                             use_auth_token=hf_token)\n",
    "    \n",
    "    if limit == -1: # map default to full examples for each skill\n",
    "        limit = len(data) # the length of all examples\n",
    "    \n",
    "    # refactor the code to have two columns, one for examples, one for reference skills\n",
    "    for idx, ( label, lat_msg) in tqdm(enumerate(zip( labels, latest_msgs)), total=limit):\n",
    "        # for idx, (each_exp, correct_skill, cot1, cot2) in enumerate(data.values.tolist()): for RAG+CoT\n",
    "        if idx < limit:\n",
    "            start = time()\n",
    "#             prompt = model.format_conversations(\n",
    "#                 messages=[{\"role\": \"user\", \"content\": input}],\n",
    "#                 functions = function_templates\n",
    "#             )\n",
    "            #prompt = generate_llama31_input(orch_prompt, str(ast.literal_eval(input)))\n",
    "            prompt = generate_llama31_input(tool_prompt, lat_msg)\n",
    "            try:\n",
    "                attention_graph = extract_llm_graph(model, prompt, max_tokens=1,latest_message = lat_msg)\n",
    "            except:\n",
    "                attention_graphs.append([])\n",
    "                responses.append(\"\")\n",
    "                graph_latency.append(-1)\n",
    "                response_latency.append(-1)\n",
    "            else:\n",
    "                attention_graph.y = label_mapping[label]\n",
    "                attention_graph.to(device(\"cpu\"))\n",
    "                attention_graphs.append(attention_graph)\n",
    "                end = time()\n",
    "                graph_latency.append(end-start)\n",
    "\n",
    "                start = time()\n",
    "    #             response = model.generate(\n",
    "    #                 prompt=prompt,\n",
    "    #                 max_tokens=25\n",
    "    #             )\n",
    "                input_tkns = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "                output = model.generate(\n",
    "                    **input_tkns,\n",
    "                    max_new_tokens=25,  # Adjust max_length as needed\n",
    "                    return_dict_in_generate=True,\n",
    "                    output_attentions=True,\n",
    "                    output_scores=True\n",
    "                )\n",
    "\n",
    "                # Decode the generated text\n",
    "                response = tokenizer.decode(output.sequences[0], skip_special_tokens=True)\n",
    "                responses.append(response)\n",
    "                end = time()\n",
    "                response_latency.append(end-start)\n",
    "            \n",
    "#         else:\n",
    "#             attention_graphs.append([])\n",
    "#             responses.append(\"\")\n",
    "#             graph_latency.append(-1)\n",
    "#             response_latency.append(-1)\n",
    "            \n",
    "    # save data\n",
    "    \n",
    "    with open(Path(output_directory, \"attention_graphs1.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(attention_graphs, f)\n",
    "    with open(Path(output_directory, \"label_mapping1.json\"), \"w\") as f:\n",
    "        json.dump(label_mapping, f)\n",
    "    DataFrame({\n",
    "        \"inputs\": inputs,\n",
    "        \"label\": labels,\n",
    "        \"responses\": responses,\n",
    "        \"graph_latency\": graph_latency,\n",
    "        \"response_latency\": response_latency\n",
    "    }).to_csv(Path(output_directory, \"responses1.csv\"), index=False, header=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #input_args = extract_arguments()\n",
    "    extract_llm_graphs(\n",
    "        input_file=input_file, \n",
    "        output_directory=output_directory, \n",
    "        #function_template_file_path=tools,\n",
    "        limit=limit\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now can we generate this for second combination?\n",
    "# With sys_prompt + chat_history + identifier + user_msg\n",
    "# prompt = generate_llama31_input(tool_prompt, df['latest_msg'][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "def filter_chat_history(msgs):\n",
    "    msgs = ast.literal_eval(msgs)\n",
    "    filt_msg = []\n",
    "    for msg in msgs:\n",
    "        if msg['role']=='user':\n",
    "            filt_msg.append(msg)\n",
    "        elif msg['role']=='assistant':\n",
    "            altered_asst_msg = {'role': 'assistant', 'content': str(msg['skill_name']) + ' - output '}\n",
    "            filt_msg.append(altered_asst_msg)\n",
    "    return str(filt_msg)\n",
    "            \n",
    "df['filtered_chat_hist'] = df.messages.apply(lambda x: filter_chat_history(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = generate_llama31_input(tool_prompt +' \\n \\n Chat history between user and assistant: \\n \\n' + df['filtered_chat_hist'][0],df['latest_msg'][0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n    You are a technical assistant, and you are a Splunk expert. You will evaluate each \\'user\\' query, user\\'s chat history and tool information to determine which tool/skill from Security assistant is fit to fulfill user\\'s request.\\nOnly respond with tool/skill name and nothing more.\\n\\nList of skills/tools available to you are: investigation_report, conversation_response, spl_writer, findings_summarizer. \\n\\nYou do not respond as \\'User\\' or pretend to be \\'User\\'. You only respond once as \\'Assistant\\'. Respond just tool/skill name based on the tool instructions provided below, user\\'s chat history and user\\'s query. \\nStop on the first request, do not continue the conversation.\\n\\nUser Chat history also contains the tool/skill name which were ran, as part of assistant response.\\n\\n### Tool Guidance: \\n\\nYou have access to four tools: \\n\\n{\\'findings_summarizer\\': {\\n   \\'description\\': \\'Findings Summarizer skill is built for \"summarizing\" or in other words \"explaining\" the findings found in an investigation. It extracts the values of the important fields from the data, such as risk object, severity, original source, mitre tactic and techniques, that are necessary to understand the findings and generates a summary based on the values of these fields. It gives its summary in two sections. Section 1 (Understanding the Findings) consists of the general summary mentioning the values of those important fields and derives a conclusion on what might be happening in the findings and what should be the next steps to investigate these findings for a Tier-1 analyst. Section 2 (MITRE Analysis) is the specific section dedicated to MITRE techniques and tactics. It includes the ids and names of these mitre annotations and derives a summary on what might be happening in the finding according to these annotations. This section also mentions what should be the next steps for analysis for a Tier-1 analyst.\\',\\n   \\'parameters\\': {\\'type\\': \\'object\\', \\'properties\\': {}, \\'required\\': []},\\n   \\'output\\': {\\'type\\': \\'string\\',\\n    \\'description\\': \\'Natural language summary of the findings.\\'},\\n         \\'examples\\': [\\'Write spl to list available source_types.\\',\\n    \\'Find the user accounts associated with failed login attempts.\\',\\n    \\'Spl query to see high risk events in last 24hrs.\\',\\n    \\'Write SPL to list all indices\\',\\n    \\'Update the SPL to use host=localhost\\',\\n    \\'How many events are in the main index\\',\\n    \\'Who has signed into host abc in the last 2 days\\']},\\n         \\n \\'investigation_report\\':{\\n   \\'description\\': \\'Investigation report skill is built to generate a report for the investigation including key information that gives a mid to high level overview of the incident. The report would display information like the investigation title, disposition, urgency, time of first finding, investigation create time. In addition, it pulls important affected/compromised assents and entities from the input data and displays these as a list. It summarizes the main MITRE tactics and techniques involved as well as gives high level summary of analyst work notes. Finally, it gives a chronological timeline of events associated with the incident including the dates, times and descriptions of activity for investigation creation, finding discovery and notes being added by the analyst.\\',\\n   \\'parameters\\': {\\'type\\': \\'object\\', \\'properties\\': {}, \\'required\\': []},\\n   \\'output\\': {\\'type\\': \\'string\\', \\'description\\': \\'A splunk incident report.\\'},\\n\\'examples\\':[\\'Give me the details about this incident.\\',\\n    \\'List all findings for a given incident.\\',\\n    \\'Generate investigation report\\',\\n    \\'report\\',\\n    \\'give me the report\\',\\n    \\'give me timeline of events\\',\\n    \\'generate timeline for this investigation\\']},\\n    \\n    \\'spl_writer\\':{\\n   \\'description\\': \"Generates an SPL query from a user\\'s natural language request when no reference SPL query is present. Returns a splunk spl query.\",\\n   \\'parameters\\': {\\'type\\': \\'object\\',\\n    \\'properties\\': {\\'intent\\': {\\'type\\': \\'string\\',\\n      \\'description\\': \"A short English sentence (50 words or less) describing the intent of the desired query. It should always start with \\'This SPL query\\'.\"}},\\n    \\'required\\': [\\'intent\\']},\\n   \\'output\\': {\\'type\\': \\'string\\',\\n    \\'description\\': \\'Natural language response with relevant chat history references.\\'},\\n  \\'examples\\': [ \\'Write spl to list available source_types.\\',\\n    \\'Find the user accounts associated with failed login attempts.\\',\\n    \\'Spl query to see high risk events in last 24hrs.\\',\\n    \\'Write SPL to list all indices\\',\\n    \\'Update the SPL to use host=localhost\\',\\n    \\'How many events are in the main index\\',\\n    \\'Who has signed into host abc in the last 2 days\\']}\\n    \\n    \\'conversation_response\\':{\\n   \\'description\\': \"Takes a user\\'s request which is not handled by [\\'finding_summarizer\\', \\'investigation_report\\', \\'spl_writer\\'] and responds with security relevant information and pertinent information already in the chat history.\",\\n   \\'parameters\\': {\\'type\\': \\'object\\',\\n    \\'properties\\': {\\'intent\\': {\\'type\\': \\'string\\',\\n      \\'description\\': \\'The most recent user specified input.\\'},\\n     \\'context\\': {\\'type\\': \\'string\\',\\n      \\'description\\': \"A list of the information in the chat history which is relevent to the user\\'s intent.\"}},\\n    \\'required\\': [\\'intent\\', \\'context\\']},\\n   \\'output\\': {\\'type\\': \\'string\\',\\n    \\'description\\': \\'Natural language response with relevant chat history references.\\'},\\n      \\'examples\\':[ \\'What are your skills?\\',\\n    \\'What does spl mean?\\',\\n    \\'How do i write an investigation report?\\',\\n    \\'What is a finding in the context of Splunk?\\',\\n    \\'What does this spl query mean?\\',\\n    \\'What is the urgency of this investigation?\\',\\n    \\'Who is the assignee for this case?\\']\\n  }\\n   \\n \\n Chat history between user and assistant: \\n \\n[{\\'role\\': \\'user\\', \\'content\\': \\'Write a summary of this notable for my boss \\\\\\'1701478355, search_name=\"Risk - 24 Hour Risk Threshold Exceeded - Rule\", all_risk_objects=\"34.215.24.225\", annotations.mitre_attack=\"T1030\", annotations.mitre_attack.mitre_tactic_id=\"TA0010\", annotations.mitre_attack.mitre_technique_id=\"T1030\", info_max_time=\"1701477600.000000000\", info_min_time=\"1701391200.000000000\", info_search_time=\"1701478354.039744000\", mitre_tactic_id_count=\"1\", mitre_technique_id_count=\"1\", normalized_risk_object=\"34.215.24.225\", risk_event_count=\"14\", risk_object=\"34.215.24.225\", risk_object_type=\"system\", risk_score=\"1120\", risk_threshold=\"100\", severity=\"critical\", orig_source=\"Network - Unusual Volume of Outbound Traffic By Src - Rule\", source_count=\"1\", orig_tag=\"modaction_result\"\\\\\\'.\\'}, {\\'role\\': \\'assistant\\', \\'content\\': \\'finding_summarizer - output \\'}, {\\'role\\': \\'user\\', \\'content\\': \\'spl to list top 10 most common error codes.\\'}, {\\'role\\': \\'assistant\\', \\'content\\': \\'spl_writer - output \\'}, {\\'role\\': \\'user\\', \\'content\\': \\'Hi!\\'}]<|eot_id|>\\n    <|start_header_id|>user<|end_header_id|>\\n\\n    Hi!<|eot_id|>\\n    <|start_header_id|>assistant<|end_header_id|>'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/pytorch/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:818: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_attentions` is. When `return_dict_in_generate` is not `True`, `output_attentions` is ignored.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a85bc3a9632c4f9ab184a90b12894a06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n",
      "  0%|                                                   | 0/174 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "/opt/conda/envs/pytorch/lib/python3.11/site-packages/transformers/generation/utils.py:2137: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  1%|▏                                          | 1/174 [00:16<47:47, 16.58s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  1%|▍                                          | 2/174 [00:32<45:34, 15.90s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  2%|▋                                          | 3/174 [00:48<46:35, 16.35s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  2%|▉                                          | 4/174 [01:04<45:11, 15.95s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  3%|█▏                                         | 5/174 [01:18<43:07, 15.31s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  3%|█▍                                         | 6/174 [01:33<43:08, 15.41s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  4%|█▋                                         | 7/174 [01:49<42:50, 15.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  5%|█▉                                         | 8/174 [02:04<42:35, 15.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  5%|██▏                                        | 9/174 [02:20<42:16, 15.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  6%|██▍                                       | 10/174 [02:35<41:56, 15.35s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  6%|██▋                                       | 11/174 [02:50<41:46, 15.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  7%|██▉                                       | 12/174 [02:57<34:01, 12.60s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  7%|███▏                                      | 13/174 [03:12<36:05, 13.45s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  8%|███▍                                      | 14/174 [03:27<37:22, 14.01s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  9%|███▌                                      | 15/174 [03:43<38:31, 14.53s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  9%|███▊                                      | 16/174 [03:58<38:53, 14.77s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 10%|████                                      | 17/174 [04:14<39:00, 14.91s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 10%|████▎                                     | 18/174 [04:31<40:33, 15.60s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 11%|████▌                                     | 19/174 [04:46<40:17, 15.59s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 11%|████▊                                     | 20/174 [05:02<39:55, 15.56s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 12%|█████                                     | 21/174 [05:19<40:45, 15.99s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 13%|█████▎                                    | 22/174 [05:36<41:18, 16.30s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 13%|█████▌                                    | 23/174 [05:53<41:44, 16.58s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 14%|█████▊                                    | 24/174 [06:09<40:41, 16.28s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 14%|██████                                    | 25/174 [06:24<39:44, 16.00s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 15%|██████▎                                   | 26/174 [06:39<38:59, 15.81s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 16%|██████▌                                   | 27/174 [06:55<38:28, 15.70s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 16%|██████▊                                   | 28/174 [07:09<36:49, 15.13s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 17%|███████                                   | 29/174 [07:27<38:58, 16.13s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 17%|███████▏                                  | 30/174 [07:45<40:00, 16.67s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 18%|███████▍                                  | 31/174 [08:02<39:58, 16.77s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 18%|███████▋                                  | 32/174 [08:20<40:38, 17.18s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 19%|███████▉                                  | 33/174 [08:38<41:01, 17.46s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|████████▏                                 | 34/174 [09:08<49:23, 21.17s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 20%|████████▍                                 | 35/174 [09:26<46:28, 20.06s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 21%|████████▋                                 | 36/174 [09:38<40:42, 17.70s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 21%|████████▉                                 | 37/174 [09:53<38:34, 16.90s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 22%|█████████▏                                | 38/174 [10:08<36:52, 16.27s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 22%|█████████▍                                | 39/174 [10:25<37:12, 16.54s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 23%|█████████▋                                | 40/174 [10:40<36:18, 16.25s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 24%|█████████▉                                | 41/174 [11:22<52:55, 23.88s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 24%|██████████▏                               | 42/174 [11:34<44:29, 20.22s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 25%|██████████▍                               | 43/174 [11:48<40:34, 18.58s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 25%|██████████▌                               | 44/174 [12:04<38:08, 17.60s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 26%|██████████▊                               | 45/174 [12:19<36:23, 16.92s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 26%|███████████                               | 46/174 [12:35<35:13, 16.51s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 27%|███████████▎                              | 47/174 [12:50<34:27, 16.28s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 28%|███████████▌                              | 48/174 [13:06<33:40, 16.04s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 28%|███████████▊                              | 49/174 [13:21<33:08, 15.91s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 29%|████████████                              | 50/174 [13:37<32:34, 15.76s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 29%|████████████▎                             | 51/174 [13:52<31:59, 15.61s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 30%|████████████▌                             | 52/174 [13:58<26:05, 12.83s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 30%|████████████▊                             | 53/174 [14:14<27:23, 13.58s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 31%|█████████████                             | 54/174 [14:29<28:22, 14.19s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 32%|█████████████▎                            | 55/174 [14:45<29:00, 14.62s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 32%|█████████████▌                            | 56/174 [15:00<29:08, 14.81s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 33%|█████████████▊                            | 57/174 [15:16<29:15, 15.01s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 33%|██████████████                            | 58/174 [15:31<29:11, 15.10s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 34%|██████████████▏                           | 59/174 [15:46<29:03, 15.16s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 34%|██████████████▍                           | 60/174 [16:02<28:51, 15.19s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 35%|██████████████▋                           | 61/174 [16:17<28:41, 15.24s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 36%|██████████████▉                           | 62/174 [16:32<28:26, 15.23s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 36%|███████████████▏                          | 63/174 [16:48<28:14, 15.27s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 37%|███████████████▍                          | 64/174 [17:03<28:05, 15.33s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 37%|███████████████▋                          | 65/174 [17:19<28:02, 15.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 38%|███████████████▉                          | 66/174 [17:25<22:52, 12.70s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 39%|████████████████▏                         | 67/174 [17:40<24:04, 13.50s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 39%|████████████████▍                         | 68/174 [17:56<24:46, 14.03s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 40%|████████████████▋                         | 69/174 [18:11<25:17, 14.46s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 40%|████████████████▉                         | 70/174 [18:27<25:34, 14.75s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 41%|█████████████████▏                        | 71/174 [18:42<25:38, 14.93s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 41%|█████████████████▍                        | 72/174 [18:57<25:37, 15.07s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 42%|█████████████████▌                        | 73/174 [19:13<25:29, 15.14s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 43%|█████████████████▊                        | 74/174 [19:28<25:20, 15.21s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 43%|██████████████████                        | 75/174 [19:43<25:13, 15.29s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 44%|██████████████████▎                       | 76/174 [19:56<23:25, 14.34s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 44%|██████████████████▌                       | 77/174 [20:02<19:19, 11.95s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 45%|██████████████████▊                       | 78/174 [20:18<20:49, 13.02s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 45%|███████████████████                       | 79/174 [20:33<21:45, 13.75s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 46%|███████████████████▎                      | 80/174 [20:48<22:19, 14.25s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 47%|███████████████████▌                      | 81/174 [21:04<22:37, 14.59s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 47%|███████████████████▊                      | 82/174 [21:19<22:46, 14.85s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 48%|████████████████████                      | 83/174 [21:35<22:52, 15.08s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 48%|████████████████████▎                     | 84/174 [21:50<22:47, 15.19s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 49%|████████████████████▌                     | 85/174 [22:00<20:03, 13.52s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 49%|████████████████████▊                     | 86/174 [22:15<20:40, 14.09s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 50%|█████████████████████                     | 87/174 [22:31<21:02, 14.51s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 51%|█████████████████████▏                    | 88/174 [22:46<21:10, 14.77s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 51%|█████████████████████▍                    | 89/174 [23:02<21:13, 14.98s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 52%|█████████████████████▋                    | 90/174 [23:08<17:20, 12.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 52%|█████████████████████▉                    | 91/174 [23:18<16:06, 11.65s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 53%|██████████████████████▏                   | 92/174 [23:34<17:32, 12.84s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 53%|██████████████████████▍                   | 93/174 [23:49<18:20, 13.59s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 54%|██████████████████████▋                   | 94/174 [24:04<18:47, 14.09s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 55%|██████████████████████▉                   | 95/174 [24:19<19:02, 14.47s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 55%|███████████████████████▏                  | 96/174 [24:30<17:13, 13.25s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 56%|███████████████████████▍                  | 97/174 [24:45<17:47, 13.86s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 56%|███████████████████████▋                  | 98/174 [25:00<18:05, 14.28s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 57%|███████████████████████▉                  | 99/174 [25:10<16:12, 12.96s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 57%|███████████████████████▌                 | 100/174 [25:26<16:49, 13.64s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 58%|███████████████████████▊                 | 101/174 [25:41<17:16, 14.20s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 59%|████████████████████████                 | 102/174 [25:50<15:07, 12.60s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 59%|████████████████████████▎                | 103/174 [26:05<15:52, 13.42s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 60%|████████████████████████▌                | 104/174 [26:21<16:19, 14.00s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 60%|████████████████████████▋                | 105/174 [26:36<16:34, 14.41s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|████████████████████████▉                | 106/174 [26:51<16:41, 14.73s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 61%|█████████████████████████▏               | 107/174 [27:07<16:38, 14.91s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 62%|█████████████████████████▍               | 108/174 [27:22<16:32, 15.03s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 63%|█████████████████████████▋               | 109/174 [27:38<16:27, 15.19s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 63%|█████████████████████████▉               | 110/174 [27:53<16:14, 15.23s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 64%|██████████████████████████▏              | 111/174 [28:08<16:03, 15.29s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 64%|██████████████████████████▍              | 112/174 [28:24<15:48, 15.31s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 65%|██████████████████████████▋              | 113/174 [28:39<15:36, 15.35s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 66%|██████████████████████████▊              | 114/174 [28:54<15:19, 15.33s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 66%|███████████████████████████              | 115/174 [29:10<15:06, 15.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 67%|███████████████████████████▎             | 116/174 [29:25<14:50, 15.36s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 67%|███████████████████████████▌             | 117/174 [29:41<14:35, 15.37s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 68%|███████████████████████████▊             | 118/174 [29:56<14:23, 15.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 68%|████████████████████████████             | 119/174 [30:12<14:07, 15.41s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 69%|████████████████████████████▎            | 120/174 [30:27<13:50, 15.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 70%|████████████████████████████▌            | 121/174 [30:43<13:39, 15.47s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 70%|████████████████████████████▋            | 122/174 [30:58<13:23, 15.44s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 71%|████████████████████████████▉            | 123/174 [31:13<13:08, 15.46s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 71%|█████████████████████████████▏           | 124/174 [31:29<12:49, 15.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 72%|█████████████████████████████▍           | 125/174 [31:44<12:36, 15.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 72%|█████████████████████████████▋           | 126/174 [32:00<12:24, 15.51s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 73%|█████████████████████████████▉           | 127/174 [32:15<12:06, 15.45s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 74%|██████████████████████████████▏          | 128/174 [32:21<09:43, 12.68s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 74%|██████████████████████████████▍          | 129/174 [32:37<10:07, 13.49s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 75%|██████████████████████████████▋          | 130/174 [32:53<10:24, 14.20s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 75%|██████████████████████████████▊          | 131/174 [33:08<10:29, 14.64s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 76%|███████████████████████████████          | 132/174 [33:24<10:25, 14.89s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 76%|███████████████████████████████▎         | 133/174 [33:40<10:20, 15.13s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 77%|███████████████████████████████▌         | 134/174 [33:55<10:08, 15.21s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 78%|███████████████████████████████▊         | 135/174 [34:10<09:54, 15.24s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 78%|████████████████████████████████         | 136/174 [34:26<09:41, 15.31s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 79%|████████████████████████████████▎        | 137/174 [34:41<09:26, 15.32s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 79%|████████████████████████████████▌        | 138/174 [34:56<09:11, 15.31s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 80%|████████████████████████████████▊        | 139/174 [35:11<08:54, 15.26s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 80%|████████████████████████████████▉        | 140/174 [35:27<08:37, 15.22s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 81%|█████████████████████████████████▏       | 141/174 [35:42<08:21, 15.19s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 82%|█████████████████████████████████▍       | 142/174 [35:57<08:05, 15.18s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 82%|█████████████████████████████████▋       | 143/174 [36:04<06:37, 12.83s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 83%|█████████████████████████████████▉       | 144/174 [36:19<06:45, 13.53s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 83%|██████████████████████████████████▏      | 145/174 [36:26<05:35, 11.55s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 84%|██████████████████████████████████▍      | 146/174 [36:41<05:53, 12.64s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 84%|██████████████████████████████████▋      | 147/174 [36:57<06:01, 13.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 85%|██████████████████████████████████▊      | 148/174 [37:12<06:01, 13.91s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 86%|███████████████████████████████████      | 149/174 [37:27<05:57, 14.29s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 86%|███████████████████████████████████▎     | 150/174 [37:42<05:49, 14.55s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 87%|███████████████████████████████████▌     | 151/174 [37:57<05:38, 14.71s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 87%|███████████████████████████████████▊     | 152/174 [38:12<05:26, 14.86s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 88%|████████████████████████████████████     | 153/174 [38:28<05:13, 14.94s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 89%|████████████████████████████████████▎    | 154/174 [38:43<05:00, 15.00s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 89%|████████████████████████████████████▌    | 155/174 [38:58<04:45, 15.05s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 90%|████████████████████████████████████▊    | 156/174 [39:13<04:31, 15.08s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 90%|████████████████████████████████████▉    | 157/174 [39:28<04:16, 15.09s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 91%|█████████████████████████████████████▏   | 158/174 [39:43<04:01, 15.12s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 91%|█████████████████████████████████████▍   | 159/174 [39:58<03:46, 15.13s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 92%|█████████████████████████████████████▋   | 160/174 [40:14<03:31, 15.13s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 93%|█████████████████████████████████████▉   | 161/174 [40:22<02:51, 13.16s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 93%|██████████████████████████████████████▏  | 162/174 [40:28<02:12, 11.04s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 94%|██████████████████████████████████████▍  | 163/174 [40:43<02:15, 12.29s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 94%|██████████████████████████████████████▋  | 164/174 [40:59<02:11, 13.16s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 95%|██████████████████████████████████████▉  | 165/174 [41:14<02:03, 13.75s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 95%|███████████████████████████████████████  | 166/174 [41:29<01:53, 14.17s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 96%|███████████████████████████████████████▎ | 167/174 [41:44<01:41, 14.47s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 97%|███████████████████████████████████████▌ | 168/174 [41:59<01:28, 14.69s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 97%|███████████████████████████████████████▊ | 169/174 [42:14<01:14, 14.83s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 98%|████████████████████████████████████████ | 170/174 [42:30<00:59, 14.93s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 98%|████████████████████████████████████████▎| 171/174 [42:45<00:45, 15.00s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 99%|████████████████████████████████████████▌| 172/174 [43:00<00:30, 15.05s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 99%|████████████████████████████████████████▊| 173/174 [43:15<00:15, 15.09s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "100%|█████████████████████████████████████████| 174/174 [43:30<00:00, 15.00s/it]\n"
     ]
    }
   ],
   "source": [
    "def extract_llm_graph(\n",
    "    model,\n",
    "    prompt:str,\n",
    "    max_tokens:int=1,\n",
    "    latest_message:str=''\n",
    "):\n",
    "    # compute decoder attention\n",
    "    \n",
    "    input_ids, outputs  = llama_extract_latent_features(prompt=prompt, max_tokens=max_tokens)\n",
    "    #specify token ids for relevant subportion of the prompt\n",
    "    #template_start_ids = search_for_target_sequence(tokenizer_llm,\"You have access to three security tools\", prompt, input_ids)\n",
    "    tokenizer_llm = tokenizer\n",
    "    \n",
    "    set_a_start_ids = search_for_target_sequence(tokenizer_llm,\"You have access to four tools\", prompt, input_ids)+2\n",
    "    set_a_end_ids = search_for_target_sequence(tokenizer_llm,\"<|eot_id|>\\n    <|start_header_id|>user<|end_header_id|>\", prompt, input_ids)-10\n",
    "    \n",
    "    set_b_end_ids = search_for_target_sequence(tokenizer_llm,\"<|start_header_id|>assistant<|end_header_id|>\", prompt, input_ids)\n",
    "    \n",
    "    \n",
    "   \n",
    "    # construct attention graph\n",
    "    return extract_latent_feature_graph(\n",
    "        input_ids=input_ids,\n",
    "        outputs=outputs,\n",
    "        position_index_a_endpoints=(set_a_start_ids, set_a_end_ids),\n",
    "        position_index_b_endpoints=(set_a_end_ids+2, set_b_end_ids-4)\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "def extract_llm_graphs(\n",
    "    input_file:str,\n",
    "    output_directory:str, \n",
    "    #function_template_file_path:str,\n",
    "    limit:int=200\n",
    ") -> None:\n",
    "    data = df\n",
    "    labels = data['label']\n",
    "    inputs = data['modified_msg']\n",
    "    filt_msgs = data['filtered_chat_hist']\n",
    "    latest_msgs = data['latest_msg']\n",
    "    responses = []\n",
    "    graph_latency = []\n",
    "    response_latency = []\n",
    "    label_mapping = {value: idx for idx, value in enumerate(list(set(labels)))}\n",
    "    \n",
    "    attention_graphs = []\n",
    "    \n",
    "    #function_templates = load_function_templates(function_template_file_path)\n",
    "    #function_templates = tools\n",
    "\n",
    "    # reference model\n",
    "    #model = llm()\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, output_attentions=True, torch_dtype=torch.float16, device_map=\"auto\",\\\n",
    "                                             use_auth_token=hf_token)\n",
    "    \n",
    "    if limit == -1: # map default to full examples for each skill\n",
    "        limit = len(data) # the length of all examples\n",
    "    \n",
    "    # refactor the code to have two columns, one for examples, one for reference skills\n",
    "    for idx, (filt_msg, label, lat_msg) in tqdm(enumerate(zip(filt_msgs, labels, latest_msgs)), total=limit):\n",
    "        # for idx, (each_exp, correct_skill, cot1, cot2) in enumerate(data.values.tolist()): for RAG+CoT\n",
    "        if idx < limit:\n",
    "            start = time()\n",
    "#             prompt = model.format_conversations(\n",
    "#                 messages=[{\"role\": \"user\", \"content\": input}],\n",
    "#                 functions = function_templates\n",
    "#             )\n",
    "            #prompt = generate_llama31_input(orch_prompt, str(ast.literal_eval(input)))\n",
    "            #prompt = generate_llama31_input(tool_prompt, lat_msg)\n",
    "            prompt = generate_llama31_input(tool_prompt +' \\n \\n Chat history between user and assistant: \\n \\n' + filt_msg,lat_msg)\n",
    "\n",
    "            try:\n",
    "                attention_graph = extract_llm_graph(model, prompt, max_tokens=1,latest_message = lat_msg)\n",
    "            except:\n",
    "                attention_graphs.append([])\n",
    "                responses.append(\"\")\n",
    "                graph_latency.append(-1)\n",
    "                response_latency.append(-1)\n",
    "            else:\n",
    "                attention_graph.y = label_mapping[label]\n",
    "                attention_graph.to(device(\"cpu\"))\n",
    "                attention_graphs.append(attention_graph)\n",
    "                end = time()\n",
    "                graph_latency.append(end-start)\n",
    "\n",
    "                start = time()\n",
    "    #             response = model.generate(\n",
    "    #                 prompt=prompt,\n",
    "    #                 max_tokens=25\n",
    "    #             )\n",
    "                input_tkns = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "                output = model.generate(\n",
    "                    **input_tkns,\n",
    "                    max_new_tokens=25,  # Adjust max_length as needed\n",
    "                    return_dict_in_generate=True,\n",
    "                    output_attentions=True,\n",
    "                    output_scores=True\n",
    "                )\n",
    "\n",
    "                # Decode the generated text\n",
    "                response = tokenizer.decode(output.sequences[0], skip_special_tokens=True)\n",
    "                responses.append(response)\n",
    "                end = time()\n",
    "                response_latency.append(end-start)\n",
    "            \n",
    "\n",
    "    \n",
    "    with open(Path(output_directory, \"attention_graphs_whist.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(attention_graphs, f)\n",
    "    with open(Path(output_directory, \"label_mapping_whist.json\"), \"w\") as f:\n",
    "        json.dump(label_mapping, f)\n",
    "    DataFrame({\n",
    "        \"inputs\": inputs,\n",
    "        \"label\": labels,\n",
    "        \"responses\": responses,\n",
    "        \"graph_latency\": graph_latency,\n",
    "        \"response_latency\": response_latency\n",
    "    }).to_csv(Path(output_directory, \"responses_whist.csv\"), index=False, header=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #input_args = extract_arguments()\n",
    "    extract_llm_graphs(\n",
    "        input_file=input_file, \n",
    "        output_directory=output_directory, \n",
    "        #function_template_file_path=tools,\n",
    "        limit=limit\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
