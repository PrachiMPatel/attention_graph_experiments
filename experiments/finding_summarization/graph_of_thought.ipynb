{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../saia-finetuning\")\n",
    "\n",
    "from functools import partial\n",
    "from typing import Any\n",
    "from tools import parse_investigation, extract_splunk_system_data, merge_dictionaries, Formatter, InformationExtractor\n",
    "\n",
    "from constants import MITRE_INSTRUCTIONS, FORMATTER_INSTRUCTIONS, METADATA_MAPPING\n",
    "from tooling.agents.executor import Executor\n",
    "from tooling.agents.state import StateGraph\n",
    "# from tooling.llm_engine.llama3_1Instruct import Llama3_1Instruct as llm\n",
    "from tooling.llm_engine.azure_oai import AzureGPT as llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify State Graph Actions and Compositions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct state graph\n",
    "action_space = StateGraph()\n",
    "\n",
    "# Instantiate LLM\n",
    "model = llm()\n",
    "\n",
    "# specify skills\n",
    "formatter = Formatter()\n",
    "information_extractor = InformationExtractor()\n",
    "\n",
    "# add actions\n",
    "action_space.add_action(action_name=\"data_chunker\", callable=parse_investigation)\n",
    "action_space.add_action(\n",
    "    action_name=f\"{information_extractor.skill_name}_mitre_information\",\n",
    "    callable=partial(information_extractor.execute_skill, model=model, instructions=MITRE_INSTRUCTIONS, output_key=\"mitre_summary\")\n",
    ")\n",
    "# action_space.add_action(\n",
    "#     action_name=f\"{information_extractor.skill_name}_system_information\",\n",
    "#     callable=partial(information_extractor.execute_skill, model=model, instructions=SPLUNK_SYSTEM_INSTRUCTIONS, temperature=0.2, output_key=\"system_summary\")\n",
    "# )\n",
    "action_space.add_action(\n",
    "    action_name=f\"{information_extractor.skill_name}_system_information\",\n",
    "    callable=partial(extract_splunk_system_data, system_data_map=METADATA_MAPPING, output_key=\"system_summary\")\n",
    ")\n",
    "action_space.add_action(\n",
    "    action_name=\"merge_context\",\n",
    "    callable=partial(merge_dictionaries, mitre_summary=\"mitre_summary\", system_summary=\"system_summary\", temperature=0.2, output_key=\"merged_summaries\")\n",
    ")\n",
    "\n",
    "action_space.add_action(\n",
    "    action_name=formatter.skill_name,\n",
    "    callable=partial(formatter.execute_skill, model=model, temperature=0.2, instructions=FORMATTER_INSTRUCTIONS, output_key=\"finding_ai_summary\")\n",
    ")\n",
    "\n",
    "# define a condition\n",
    "def state_contains_keys(keys:list[str], context:dict[str,Any]) -> bool:\n",
    "    return set(context).issuperset(keys)\n",
    "\n",
    "# add conditional composition\n",
    "# chunk data and send subcontexts to information extractors\n",
    "action_space.add_composition(\n",
    "    action_name_0=\"data_chunker\",\n",
    "    action_name_1=f\"{information_extractor.skill_name}_mitre_information\",\n",
    "    argument_map={\"mitre_subcontext\": \"context\"},\n",
    ")\n",
    "action_space.add_composition(\n",
    "    action_name_0=\"data_chunker\",\n",
    "    action_name_1=f\"{information_extractor.skill_name}_system_information\",\n",
    "    argument_map={\"system_subcontext\": \"investigation_data\"},\n",
    ")\n",
    "# action_space.add_composition(\n",
    "#     action_name_0=\"data_chunker\",\n",
    "#     action_name_1=f\"{information_extractor.skill_name}_system_information\",\n",
    "#     argument_map={\"system_subcontext\": \"context\"},\n",
    "# )\n",
    "\n",
    "# TODO need to define logic for a skill to specify multiple skills must finish prior to executing\n",
    "# In this notebook we avoid this due to the BFS execution logic and early termination\n",
    "required_keys = [\"mitre_subcontext\", \"system_subcontext\"]\n",
    "action_space.add_composition(\n",
    "    action_name_0=f\"{information_extractor.skill_name}_mitre_information\",\n",
    "    action_name_1=\"merge_context\",\n",
    "    condition=partial(state_contains_keys, keys=[\"mitre_summary\", \"system_summary\"])\n",
    ")\n",
    "action_space.add_composition(\n",
    "    action_name_0=f\"{information_extractor.skill_name}_system_information\",\n",
    "    action_name_1=\"merge_context\",\n",
    "    argument_map={\"mitre_summary\": \"mitre_summary\", \"system_summary\": \"system_summary\"},\n",
    "    condition=partial(state_contains_keys, keys=[\"mitre_summary\", \"system_summary\"])\n",
    ")\n",
    "action_space.add_composition(\n",
    "    action_name_0=\"merge_context\",\n",
    "    action_name_1=formatter.skill_name,\n",
    "    argument_map={\"merged_summaries\": \"context\"},\n",
    "    condition=partial(state_contains_keys, keys=[\"merged_summaries\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "graph = action_space.graph\n",
    "pos = nx.planar_layout(action_space.graph)\n",
    "nx.draw_networkx_nodes(graph, pos)\n",
    "nx.draw_networkx_edges(graph, pos)\n",
    "nx.draw_networkx_labels(graph, pos, font_size=8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "FINDING_FILES = [\n",
    "    \"/mount/splunka100groupstorage/a100-fs-share1/lbetthauser/scripts/finding_summarization/sample_findings/July23_8_Investigations_Analyst_Notes.json\",\n",
    "    \"/mount/splunka100groupstorage/a100-fs-share1/lbetthauser/scripts/finding_summarization/sample_findings/July25_10_Investigations_Analyst_Notes.json\",\n",
    "    \"/mount/splunka100groupstorage/a100-fs-share1/lbetthauser/scripts/finding_summarization/sample_findings/noah3_investigations.json\",\n",
    "    \"/mount/splunka100groupstorage/a100-fs-share1/lbetthauser/scripts/finding_summarization/sample_findings/sample_input.json\",\n",
    "]\n",
    "investigations = []\n",
    "for file in FINDING_FILES:\n",
    "    with open(file, \"r\") as f:\n",
    "        _data = json.load(f)\n",
    "        if isinstance(_data, list):\n",
    "            investigations += _data\n",
    "        elif isinstance(_data, dict):\n",
    "            investigations.append(_data)\n",
    "        else:\n",
    "            print(f\"could not parse file: {file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Executor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate executor\n",
    "executor = Executor()\n",
    "executor.add_state_graph(action_space)\n",
    "\n",
    "# construct initial state if applicable\n",
    "INVESTIGATION = investigations[0]\n",
    "state = {\n",
    "    \"investigation\": INVESTIGATION\n",
    "}\n",
    "\n",
    "executor.augment_state(state)\n",
    "executor.set_entry_point(action_name=\"data_chunker\")\n",
    "\n",
    "# run executor\n",
    "terminal_state = executor.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final answer\n",
    "print(terminal_state[\"finding_ai_summary\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs\n",
    "print(terminal_state[\"system_summary\"])\n",
    "print(terminal_state[\"mitre_summary\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
